\section{Problem Set \#3}

\begin{exercise}
    Let \(X_{1}, \ldots, X_{n} \stackrel{i.i.d.}{\sim} f(x \mid \eta)\), where \(\{f(x \mid \eta)\}_{\eta \in \mathcal{E}}\) is a full rank, \(k\)-parameter exponential family. Show that
    \begin{enumerate}[(a)]
        \item \(\{f(\mathbf{x} \mid \eta)\}_{\eta \in \mathcal{E}}\) is a full rank, \(k\)-parameter exponential family; 
        \item \(\sum_{i=1}^{n} T\left(X_{i}\right)\) is minimal sufficient for \(\eta\). 
        \item Conclude that, under i.i.d. measurements, the dimension of the minimal sufficient statistic is independent of the sample size \(n\). In other words, canonical exponential families display a strong dimension reduction property. 
    \end{enumerate}
    \rmk{1} One question of interest is: what other families of distributions permit dimension reduction in the sense described in part (c) of this problem? The Darmois-Koopman-Pitman theorem, stated below without proof, answers this question. Under certain regularity assumptions, only exponential families have this property!
    
    \thm{1} (Darmois-Koopman-Pitman theorem; Theorem 1.6.18 in Lehmann and Casella (1998), p.40). Suppose that the following assumptions hold.
    \begin{enumerate}[(a)]
        \item \(X_{1}, \ldots, X_{n} \stackrel{i.i.d. }{\sim} f\left(x_{1} \mid \theta\right)\), where \(f\left(x_{1} \mid \theta\right)\) is a.c.; 
        \item \(\operatorname{supp}_{\theta} f\left(x_{1} \mid \theta\right)\) is an interval; 
        \item for \(f(\boldsymbol{x} \mid \theta)=\prod_{i=1}^{n} f\left(x_{i} \mid \theta\right)\), there exists a continuous \(k\)-dimensional sufficient statistic, where \(k<n\). 
    \end{enumerate}
    Then,
    \begin{enumerate}[(i)]
        \item if \(k=1\), then there are functions \(\eta_{1}(\cdot), B(\cdot), h(\cdot)\) such that
        \[
        f(\boldsymbol{x} \mid \theta)=h(\boldsymbol{x}) \exp \left\{\eta_{1}(\theta) T_{1}(\boldsymbol{x})-B(\theta)\right\}. 
        \]
        \item if \(k \geq 2\) and \(f\left(x_{1} \mid \theta\right)\) has continuous partial derivatives with respect to \(x_{1}\), then there exist functions \(\eta_{i}(\cdot), B(\cdot), h(\cdot)\) such that
        \[
        f(\boldsymbol{x} \mid \theta)=h(\boldsymbol{x}) \exp \left\{\sum_{i=1}^{s} \eta_{i}(\theta) T_{i}(\boldsymbol{x})-B(\theta)\right\}, \quad s \leq k .
        \]
    \end{enumerate}
\end{exercise}

\begin{solution}
    \begin{enumerate}[(a)]
        \item Knowing that 
        \[
            f(x_i|\eta)=h(x_i)\exp\left(\eta^T T(x_i)-A(\eta)\right). 
        \]
        And the space for $\eta$ is $k$-dimension. So, \[
            f(x|\eta)=\prod_{i=1}^n h(x_i)\cdot\exp\left(\eta^T\sum_{i=1}^nT(x_i)-nA(\eta)\right). 
        \]
        Its parameter space is also $k$-dimension. So, $f(x|\eta)$ is full rank. 
        \item For $x\neq y$, 
        \[
            \frac{f(x|\eta)}{f(y|\eta)}=\exp\left(\eta^T\left(\sum_{i=1}^nT(x_i)-\sum_{i=1}^nT(y_i)\right)\right). 
        \]
        So, $\sum_{i=1}^nT(x_i)$ is minimal sufficient. 
        \item Because the parameter space is $k$-dimension, the dimension for minimal sufficient statistic $\sum_{i=1}^nx_i$ is at most $k$-dimension, which is independent with $n$. 
    \end{enumerate}
\end{solution}

\begin{exercise}
    Let \(Z_{1}, \ldots, Z_{n} \stackrel{i.i.d. }{\sim} F(z)\), where \(F(z)\) is a c.d.f. that is continuous at zero. Let \(X_{i}=\sigma Z_{i}\), \(i=1, \ldots, n\). Show that \(\left(X_{1} / X_{n}, \ldots, X_{n-1} / X_{n}\right)^{T}\) is ancillary for \(\sigma\).
\end{exercise}

\begin{solution}
    \[
        \frac{X_i}{X_n}=\frac{Z_i}{Z_n}
    \]
    it doesn't depend on $\sigma$. So, \(\left(X_{1} / X_{n}, \ldots, X_{n-1} / X_{n}\right)^{T}\) is ancillary for \(\sigma\). 
\end{solution}

\begin{exercise}
    Let \(X_{1}, \ldots, X_{n} \stackrel{i.i.d. }{\sim} U[\theta, \theta+1]\), \(\theta \in \mathbb{R}\) (i.e., the location family generated by a \(U[0,1]\) distribution). 
    \begin{enumerate}
        \item Show that the vector and univariate statistics
        \[
        T(\mathbf{X})=\left(X_{(n)}-X_{(1)}, \frac{X_{(1)}+X_{(n)}}{2}\right)^{T}, \quad V(\mathbf{X})=X_{(n)}-X_{(1)}
        \]
        are, respectively, minimal sufficient and ancillary for \(\theta\). 
        \item Conclude that \(T(\mathbf{X})\) and \(V(\mathbf{X})\) are dependent (a paradoxical conclusion!). 
    \end{enumerate}
\end{exercise}

\begin{solution}
    \begin{enumerate}
        \item $p(x_i|\theta)=\mathbf{1}_{\theta\leqslant x_i\leqslant \theta+1}$, 
        \[
            p(x)=\prod_{i=1}^n\mathbf{1}_{\theta\leqslant x_i\leqslant \theta+1}=\mathbf{1}_{\theta\leqslant x_{(1)}, x_{(n)}\leqslant \theta+1}. 
        \]
        
        Let $R=X_{(n)}-X_{(1)}$, $M=\frac{X_{(1)}+X_{(n)}}{2}$. Then $X_{(1)}=\frac{2M-R}{2}$, $X_{(n)}=\frac{2M+R}{2}$. The joint density function for $X_{(1)}$ and $X_{(n)}$ is 
        \[
            g(x_{(1)}, x_{(n)})=n(n-1)(x_{(n)}-x_{(1)})^{n-2}. 
        \]
        Replace $X_{(1)}, X_{(n)}$ with $M$ and $R$. We can get 
        \[
            g(M,R)=n(n-1)(R)^{n-2}. 
        \]
        So, \[
            g(M)=n, \qquad g(R)=n(n-1)R^{n-2}(1-R). 
        \]
        Both of them are independent with $\theta$. So, they are ancillary. 

        \item Obviously, $g(M)\cdot g(R)\neq g(M,R)$. 
    \end{enumerate}
    
\end{solution}

\begin{exercise}
    Let \(X_{1}, \ldots, X_{n} \stackrel{i.i.d. }{\sim} \mathcal{N}\left(\sigma, \sigma^{2}\right), \sigma>0, n \geq 2\). As seen in class,
\[
T(\mathbf{X})=\left(\sum_{i=1}^{n} X_{i}^{2}, \sum_{i=1}^{n} X_{i}\right)^{T}
\]
is minimal sufficient. We now show that \(T(\mathbf{X})\) is not complete. 
    \begin{enumerate}[(a)]
        \item Let \(\delta_{1}(\mathbf{X})=\bar{X}\). Show that \(\mathbb{E}\left(\delta_{1}(\mathbf{X}) \mid \sigma\right)=\sigma\). 
        \item Let
        \[
        \delta_{2}(\mathbf{X})=\sqrt{\frac{n-1}{2}} \frac{\Gamma((n-1) / 2)}{\Gamma(n / 2)} \sqrt{S^{2}}
        \]
        Show that \(\mathbb{E}\left(\delta_{2}(\mathbf{X}) \mid \sigma\right)=\sigma\). 
        \item Set \(g\left(t_{1}, t_{2}\right)=t_{1}-t_{2}\) and conclude that \(T(\mathbf{X})\) is not complete. 
    \end{enumerate}
\end{exercise}

\begin{solution}
    \begin{enumerate}[(a)]
        \item From the Large Number Theorem, we know that $\bar{X}\sim N(\sigma, \sigma^2/n)$. So, 
        \[
            \mathbf{E}(\bar{X})=\sigma. 
        \]
        \item \[S^2=\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar{X})^2\]
        \[
            \frac{(n-1)S^2}{\sigma^2}=\sum_{i=1}^n(\frac{X-\bar{X}}{\sigma})^2\sim\chi_n^2. 
        \]
        So, $\mathbf{E}(S)=\sigma\sqrt{\frac{2}{n-1}}\cdot \frac{\Gamma(n/2)}{\Gamma((n-1)/2)}$. Then, we have
        \[
            \mathbf{E}(\delta_2(X)|\sigma)=\sigma. 
        \]
        \item $\mathbf{E}(\delta_1(X)-\delta_2(X))=0$, but $\Pr(\delta_1(X)-\delta_2(X))=\Pr(\bar{X}-\delta_2(X))\neq 1$. 
    \end{enumerate}
\end{solution}

\begin{exercise}
    Let \(Y_{1}, \ldots, Y_{n}\) be independent random variables, each with distribution \(Y_{i} \sim B\left(n_{i}, p_{i}\right)\), \(i=1, \ldots, n\). To fix ideas, make the additional assumptions
    \begin{itemize}
        \item \(n=2, n_{1}=2, n_{2}=1 ;\)
        \item \(p_{i}=p_{i}\left(x_{i}\right)=1-e^{-\theta x_{i}} \in[0,1), \theta>0, x_{i} \geq 0, i=1,2\) (example: \(x_{i}\) represents the chosen level of a medication, \(p_{i}\left(x_{i}\right)\) is the probability a patient recovers and \(Y_{i}\) is the number of patients who recover); 
        \item \(x_{1}:=1<2=: x_{2}\).
    \end{itemize}
    \begin{enumerate}[(a)]
        \item Express \(\eta_{2}(\theta)\) as a function of \(\eta_{1}(\theta)\) and conclude that the model forms a curved exponential family. 
        \item Show that \(\mathbf{Y}=\left(Y_{1}, Y_{2}\right)^{T}\) is minimal sufficient for \(\theta\). 
        \item Use the statistic \(1_{\left\{Y_{1}=0\right\}}-1_{\left\{Y_{2}=0\right\}}\) to conclude that \(\mathbf{Y}\) is not complete. 
        \item Please solve the following conundrum. The technical condition used in part (b) implies the minimal sufficiency of \(T(\mathbf{Y})=\mathbf{Y}\) and also that \(\mathcal{E}\) is not contained in a unidimensional hyperplane. Therefore, by Problem set \#1, \(\operatorname{int}(\mathcal{E}) \neq \emptyset\), which, in turn, implies the completeness of \(T(\mathbf{Y})=\mathbf{Y}\). What is going on? 
    \end{enumerate}
\end{exercise}

\begin{solution}
    \begin{enumerate}[(a)]
        \item \[f(y_1=k_1)=\binom{2}{k_1}p_1^{k_1}(1-p_1)^{2-k_1}, \]
        \[f(y_2=k_2)=\binom{1}{k_2}p_2^{k_2}(1-p_2)^{1-k_2}=p_2^{k_2}(1-p_2)^{1-k_2}. \]
        \item \[
            \begin{aligned}
                &\quad\, f(y_1=k_1, y_2=k_2)\\
                &=f(y_1=k_1)f(y_2=k_2)\\
                &=\binom{2}{k_1}\exp\left(k_1\left(\ln\left(1-e^{-\theta}\right)+\theta\right)+k_2\left(\ln\left(1-e^{-2\theta}\right)+2\theta\right)-4\theta\right). 
            \end{aligned}
        \]
        \[
            \begin{aligned}
                &\frac{f(y_1=k_1, y_2=k_2)}{f(y_1=m_1, y_2=m_2)}\\
                ={}&{}\frac{\binom{2}{k_1}}{\binom{2}{m_1}}\exp\left((k_1-m_1)\left(\ln\left(1-e^{-\theta}\right)+\theta\right)+(k_2-m_2)\left(\ln\left(1-e^{-2\theta}\right)+2\theta\right)\right), 
            \end{aligned}
        \]
        If we want this ratio doesn't depend on $\theta$, then $k_1=m_1$, $k_2=m_2$. $Y=(Y_1,Y_2)$ is minimal statistic. 
        \item \[
            \mathbf{E}(1_{Y_1=0})=\Pr(Y_1=0)=(1-p_1)^2=e^{-2\theta}. 
        \]
        \[
            \mathbf{E}(1_{Y_2=0})=\Pr(Y_2=0)=(1-p_2)^2=e^{-2\theta}. 
        \]
        So, $\mathbf{E}(1_{Y_1=0}-1_{Y_2=0})=\mathbf{E}(f(Y))=0$. But $\Pr(1_{Y_1=0}-1_{Y_2=0})\neq 1$. 
        \item I think the $\mathcal{E}$ is the parameter space of canonical form, and because it is curved, int$(\mathcal{E})=\emptyset$. 
    \end{enumerate}
\end{solution}

\begin{exercise}
    Let \(X_{1}, \ldots, X_{n} \stackrel{i.i.d. }{\sim} \operatorname{Poi}(\lambda)\), \(\lambda>0\). Prove directly - i.e., without resorting to exponential families - that the statistic \(T(\mathbf{X})=\sum_{i=1}^{n} X_{i}\) is complete for \(\lambda\). 
\end{exercise}

\begin{solution}
    Knowing that  
    \[
        \bar{X}\sim N\left(\lambda, \frac{\lambda}{n}\right), \qquad T=\sum_{i=1}^nX_i\sim N\left(n\lambda, n\lambda\right). 
    \]
    For some function $f(T)$, 
    \[
        \mathbf{E}(f(T))=\sum f(t)\frac{1}{\sqrt{2\pi n\lambda}}e^\frac{-(t-n\lambda)^2}{2n\lambda}=0. 
    \]
    Because $\frac{1}{\sqrt{2\pi n\lambda}}e^\frac{-(t-n\lambda)^2}{2n\lambda}>0$. So, $f(t)=0$, i.e. $\Pr(f(t)=0)=1$, a.s. 
\end{solution}

\begin{exercise}
    Let \(Y_{1}, \ldots, Y_{n} \stackrel{i.i.d. }{\sim} E(\eta, 1), \eta \in \mathbb{R}\), namely, a shifted exponential distribution, which has density
    \[
    f_{Y_{1}}(y)=e^{-(y-\eta)} 1_{(\eta, \infty)}(y), \quad y \in \mathbb{R} .
    \]
    Let \(X_{i}=e^{-Y_{i}}, i=1, \ldots, n\), and \(\theta=e^{-\eta}\)
    \begin{enumerate}
        \item Show that \(X_{1}, \ldots, X_{n} \stackrel{i.i.d. }{\sim} U[0, \theta], \theta>0\). 
        \item Conclude that \(X_{(n)}\) is complete for \(\theta\) or \(\eta\). 
        \item Now consider \(Y_{(1)}=-\log X_{(n)}\). Show that \(Y_{(1)}\) is complete for \(\eta\). 
    \end{enumerate}
\end{exercise}

\begin{solution}
    \begin{enumerate}
        \item \[
            F(y\leqslant Y)=\int_\eta^ye^{-t+\eta}\der t= 1-e^{-y+\eta}. 
        \]
        \[
            \begin{aligned}
                \Pr(x_i\leqslant X_i)&=\Pr(e^{-y_i}\leqslant X_i)\\
                &=\Pr(y_i\geqslant -\ln(X_i))\\
                &=e^{\ln(X_i)+\eta}=\frac{1}{\theta}X_i. 
            \end{aligned}
        \]
        So, we can know that \(f(x_i)=\frac{1}{\theta}\). 
        \item $f_{X_{(n)}}(x)=nx^{n-1}\theta^{-n}$, $0<x<\theta$. If for some function $g(T)$, $\mathbf{E}(g(T))=0$, then 
        \[
            \begin{aligned}
                0 &=\frac{\der}{\der  \theta} \mathbf{E} g(T) \\
                &=\frac{\der}{\der  \theta} \int_{0}^{\theta} g(t) n t^{n-1} \theta^{-n} \der  t \\
                &=\left(\theta^{-n}\right) \frac{\der}{\der  \theta} \int_{0}^{\theta} n g(t) t^{n-1} \der  t+\left(\frac{\der}{\der  \theta} \theta^{-n}\right) \int_{0}^{\theta} n g(t) t^{n-1} \der  t\\
                &=\frac{ng(\theta)}{\theta}. 
            \end{aligned}
        \]
        And because $\theta>0$, $n>0$. So, $g(\theta)=0$. Then, $T$ is complete. 
        \item They are equivalent, and $Y_{(1)}$ is complete for $\eta$. 
    \end{enumerate}
\end{solution}

\begin{exercise}
    Let \(\mathcal{P}\) be a family of probability measures. If \(T\) is complete for \(\mathcal{P}\) and \(U\) is equivalent to \(T\), show that \(U\) is complete for \(\mathcal{P}\). 
\end{exercise}

\begin{solution}
    For $T$, if it is complete, then for some function $g(T)$, 
    \[
        \int g(t)f(t)\der t=0 \Rightarrow \Pr(g(t)=0)=1. 
    \]
    Now, $T=h(U)$, and let $l(u)=g\circ h(u)$, we can get that 
    \[
        \int l(u)f(u)\der u=0 \Rightarrow \Pr(l(u)=0)=1. 
    \]
    So, $U$ is also complete. 
\end{solution}

\begin{exercise}
    Let \(X_{1}, \ldots, X_{n} \stackrel{i.i.d. }{\sim} \mathcal{N}\left(\mu, \sigma^{2}\right)\), \(\mu \in \mathbb{R}, \sigma^{2}>0 .\) Show that \(\bar{X}\) and \(S^{2}\) are independent. 
    
    (\rmk{2}: this classical result can be shown by means of characteristic functions. However, in the context of our course, a much shorter proof can be provided). 
\end{exercise}

\begin{solution}
    We know that $(\bar{X}, S^2)$ are sufficient. And $\bar{X}\sim N(\mu,\sigma^2/n)$. So, $\bar{X}$ is complete for $\mu$ (Because $N(\mu,\sigma^2/n)$ is full rank exponential family). And $S^2$ only depends on $\sigma^2$ (Because $\frac{(n-1)S^2}{\sigma^2}=\sum_{i=1}^n(\frac{X-\bar{X}}{\sigma})^2\sim\chi_n^2$), it is a ancillary statistic. So, from Basu's theorem, they are independent. 
\end{solution}

