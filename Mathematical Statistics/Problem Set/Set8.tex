\chapter{}

\begin{ex}
    \rmk{1} this problem reinterprets a nonrandomized test as a randomized one. 

    Given a family of probability measures 
    \(\mathcal{P}\), consider the \(\phi_{1}(\mathbf{x}):=1_{R}(\mathbf{x}), R \in \mathcal{B}\left(\mathbb{R}^{n}\right)\), called ``test 1". Now consider the following procedure (called ``test 2" ): observe \(\mathbf{x}\), and
    \[\left\{\begin{array}{l}\text { if } \mathbf{x} \in R, \text { flip an independent coin } Y \sim B(1,1) \text { and reject } H_{0} \text { if and only if } Y=1 \\ \text { if } \mathbf{x} \in R^{c}, \text { flip an independent coin } Y \sim B(1,0) \text { and reject } H_{0} \text { if and only if } Y=1\end{array}\right.\]
    Conclude that the tests 1 and 2 are equal \(\mathcal{P}\)-a.s. 
\end{ex}

\begin{ex}
    \rmk{2} in this problem, we calculate the size of a randomized test. 

    Assume that we want to decide between the following two distributions for a random variable \(X\) :
    \[
        H_{0}: \mathbb{P}_{0}(X=x)=\left\{\begin{array}{ll}
        0.05, & \text { if } x=2 ; \\
        0.10, & \text { if } x=1 ; \\
        0.85, & \text { if } x=0 .
        \end{array} \quad H_{1}: \mathbb{P}_{1}(X=x)= \begin{cases}0.40, & \text { if } x=2 \\
        0.50, & \text { if } x=1 \\
        0.10, & \text { if } x=0\end{cases}\right.
    \]
    Consider the (randomized) test \((n=1)\)
    \[
        \phi(x)=\left\{\begin{array}{cl}
        1, & \text { if } x=2 \\
        0.5, & \text { if } x=1 \\
        0, & \text { if } x=0
        \end{array}\right.
    \]
    This means that, once we observe \(x\), we flip an independent coin \(Y \sim B(1, \phi(x))\) and reject \(H_{0}\) if and only if \(Y=1\). Prove that \(\phi\) is a size \(0.10\) test, i.e.,
    \[
        \mathbb{E}_{0} \phi(X)=0.10
    \]
    (note that \(H_{1}\) plays no role). 
\end{ex}

\begin{ex}
    Let \(X_{1}, \ldots, X_{n} \stackrel{\text { i.i.d. }}{\sim} \mathcal{N}\left(\mu, \sigma_{0}^{2}\right)\), where \(\sigma_{0}^{2}\) is known. Find the MP level \(\alpha\) test for
    \[
        H_{0}: \mu=\mu_{0} \quad \text { vs } \quad H_{1}: \mu=\mu_{1}
    \]
    when \(\mu_{1}<\mu_{0}\). 
\end{ex}

\begin{ex}
    A lot of items contains the proportion \(p\) of defectives. The lot is large enough so that it is reasonable to assume individual items selected at random without replacement are independent Bernoulli trials with parameter \(p\). Suppose \(n\) items are selected at random. We want to test
    \[
        H_{0}: p=0.1 \quad \text { vs } \quad H_{1}: p=0.2
    \]
    Considering only nonrandomized tests, prove that the MP level \(\alpha\) test rejects in the region
    \begin{equation}
        \label{eq:8.4}
        \left\{\mathbf{x}: \sum_{i=1}^{n} x_{i}>c(\alpha)\right\}. 
    \end{equation}
    for appropriate values of \(\alpha\) (n.b.: there are only finitely many values of \(\alpha\) such that the nonrandomized test (\ref{eq:8.4}) is \(\mathrm{MP})\). Indicate how one calculates \(c\) as a function of \(\alpha\). 
\end{ex}

\begin{ex}
    Let \(X_{1}, \ldots, X_{n} \stackrel{\text { i.i.d. }}{\sim} \exp (\lambda)\). For \(\lambda_{0}>0\), we want to test
    \[
        H_{0}: \lambda \leq \lambda_{0} \quad \text { vs } \quad H_{1}: \lambda>\lambda_{0} .
    \]
    Find the UMP level \(\alpha\) test. 
\end{ex}


\begin{ex}
    Let \(X_{1}, \ldots, X_{n} \stackrel{\text { i.i.d. }}{\sim} \mathcal{N}\left(\mu, \sigma_{0}^{2}\right)\), where \(\sigma_{0}^{2}\) is known. Prove that the family of distributions has a MLR with respect to \(\bar{X}\). 
\end{ex}

\begin{ex}
    Let \(\mathcal{P}\) be a one-parameter canonical exponential family generated by \(T(x)\) and \(\mathcal{E}=\operatorname{int} \mathcal{E} \neq \emptyset\). Show that \(\mathcal{P}\) has a MLR with respect to \(T(x)\). 
\end{ex}

\begin{ex}
    Suppose that the assumptions of the Neyman-Pearson lemma hold for hypotheses of the form
    \[
        H_{0}: \theta=\theta_{0} \quad \text { vs } \quad H_{1}: \theta=\theta_{1} .
    \]
    For a given \(\alpha\), let \(\pi\left(\theta_{1}\right)\) be the power of the MP test. We would like to show that \(\alpha<\pi\left(\theta_{1}\right)\). 
    \begin{enumerate}[(a)]
        \item Define the test \(\psi(\mathbf{x}) \equiv \alpha\), i.e., irrespective of \(\mathbf{x}\), it rejects \(H_{0}\) with probability \(\alpha\) (remark: this is a randomized test). Conclude that \(\alpha \leq \pi\left(\theta_{1}\right)\). 
        \item Assume that \(\alpha=\pi\left(\theta_{1}\right)\). Conclude that \(\psi(\mathbf{x})\) is \(\mathrm{MP}\), and arrive at a contradiction with one of the assumptions of the Neyman-Pearson lemma. 
    \end{enumerate}
\end{ex}

\begin{ex}
    (in this problem, we show the Karlin-Rubin theorem)

    Suppose that the family \(\mathcal{P}=\) \(\{f(\mathbf{x} \mid \theta)\}_{\theta \in \Theta}\) has MLR with respect to \(T(\mathbf{x})\), and we are willing to test the hypothesis
    \[
        H_{0}: \theta \leq \theta_{0} \quad \text { vs } \quad H_{1}: \theta>\theta_{0}. 
    \]
    \begin{enumerate}[(a)]
        \item Consider first the simple hypothesis
        \[
        H_{0}: \theta=\theta_{0} \quad \text { vs } \quad H_{1}: \theta=\theta_{1}>\theta_{0}
        \]
        Show that the test that rejects for large values of \(T(\mathbf{x})\) is MP. 
        \item For a fixed, pre-chosen \(\alpha\), show that there are \(c\) and \(\gamma\) such that
        \[
        \phi(\mathbf{x})=\left\{\begin{array}{ll}
        1, & \text { if } T(\mathbf{x})>c \\
        0, & \text { if } T(\mathbf{x})<c ; \\
        \gamma, & \text { if } T(\mathbf{x})=c
        \end{array} \quad \mathbb{E}_{\theta_{0}} \phi(\mathbf{X})=\alpha\right.
        \]
        (hint: use the argument in the existence part of the Neyman-Pearson lemma).
        \item Conclude that the test obtained in (b) is also MP for testing \(\theta^{\prime}\) against \(\theta^{\prime \prime}, \theta^{\prime}<\theta^{\prime \prime}\), at level \(\alpha^{\prime}=\pi\left(\theta^{\prime}\right)\). 
        \item Show that the power function
        \[
            \pi\left(\theta^{\prime}\right)=\mathbb{E}_{\theta^{\prime}} \phi(\mathbf{X})
        \]
        is strictly increasing (hint: use the previous problem). 
        \item Let \(\psi(\mathbf{X})\) be any test, and consider the condition
        \item 
        \begin{equation}
            \label{eq:8.9.e.1}
            \mathbb{E}_{\theta} \psi(\mathbf{X}) \leq \alpha, \quad \theta \leq \theta_{0}. 
        \end{equation}
        Conclude that \(\psi(\mathbf{X})\) also satisfies
        \begin{equation}
            \label{eq:8.9.e.2}
            \mathbb{E}_{\theta_{0}} \psi(\mathbf{X}) \leq \alpha. 
        \end{equation}
        \item Conclude that \(\phi(\mathbf{X})\) satisfies the condition (\ref{eq:8.9.e.1}) and that it is MP (with respect to \(\theta_{1}\) ) in the broader class of tests that satisfy (\ref{eq:8.9.e.2}). 
        \item Conclude that \(\phi(\mathbf{X})\) is UMP against \(H_{1}\). 
        
        (hint: use (f) and the fact that \(\phi(\mathbf{X})\) is independent of the particular \(\theta_{1}\) chosen). 
    \end{enumerate}
\end{ex}


\begin{ex}
    Let \(X_{1}, \ldots, X_{n} \stackrel{\text { i.i.d. }}{\sim} \mathcal{N}\left(\mu, \sigma^{2}\right)\), where both parameters are unknown. Prove that the GLR tests have the form
    \[
        \begin{array}{ccc}
        \hline H_{0} & H_{1} & R \\
        \hline \mu \leq \mu_{0} & \mu>\mu_{0} & \bar{x}>\mu_{0}+\frac{s}{\sqrt{n}} t_{\alpha} \\
        \mu \geq \mu_{0} & \mu<\mu_{0} & \bar{x}<\mu_{0}-\frac{s}{\sqrt{n}} t_{\alpha} \\
        \mu=\mu_{0} & \mu \neq \mu_{0} & \left|\frac{\bar{x}-\mu_{0}}{s / \sqrt{n}}\right|>t_{\alpha / 2} \\
        \hline
        \end{array}
    \]
\end{ex}

\begin{ex}
    Let \(X_{1}, \ldots, X_{n} \stackrel{\text { i.i.d. }}{\sim} \mathcal{N}\left(\mu, \sigma^{2}\right)\), where both parameters are unknown. Prove that the GLR tests have the form
    \begin{center}
        \begin{tabular}{ccc}
            \hline\(H_{0}\) & \(H_{1}\) & \(R\) \\
            \hline\(\sigma^{2} \leq \sigma_{0}^{2}\) & \(\sigma^{2}>\sigma_{0}^{2}\) & \(\frac{(n-1) s^{2}}{\sigma_{0}^{2}}>\chi_{n-1}^{2}(\alpha)\) \\
            \(\sigma^{2} \geq \sigma_{0}^{2}\) & \(\sigma^{2}<\sigma_{0}^{2}\) & \(\frac{(n-1) s^{2}}{\sigma_{0}^{2}}<\chi_{n-1}^{2}(1-\alpha)\) \\
            \(\sigma^{2}=\sigma_{0}^{2}\) & \(\sigma^{2} \neq \sigma_{0}^{2}\) & \(\left\{\frac{(n-1) s^{2}}{\sigma_{0}^{2}}<\chi_{n-1}^{2}(1-\alpha / 2)\right\} \cup\left\{\frac{(n-1) s^{2}}{\sigma_{0}^{2}}>\chi_{n-1}^{2}(\alpha / 2)\right\}\) \\
            \hline
        \end{tabular}
    \end{center}
\end{ex}

\begin{ex}
    Suppose we have two independent samples: \(X_{1}, \ldots, X_{n} \stackrel{\text { i.i.d. }}{\sim} \exp \left(\lambda_{1}\right), Y_{1}, \ldots, Y_{m} \stackrel{\text { i.i.d. }}{\sim} \exp \left(\lambda_{2}\right)\). 
    \begin{enumerate}[(a)]
        \item Find the GLR test for the hypotheses
        \[
        H_{0}: \lambda_{1}=\lambda_{2} \quad \text { vs } \quad H_{1}: \lambda_{1} \neq \lambda_{2}. 
        \]
        (remark: note that the definition of GLR tests does encompass this two-sample case). 
        \item Show that the test in part (a) can be based on the statistic
        \[
        T(\mathbf{X}, \mathbf{Y})=\frac{\sum_{i=1}^{n} X_{i}}{\sum_{i=1}^{n} X_{i}+\sum_{j=1}^{m} Y_{j}}. 
        \]
        \item Find the distribution of \(T(\mathbf{X}, \mathbf{Y})\) when \(H_{0}\) is true. 
    \end{enumerate}
    (hint: for (b), express \(\Lambda(\mathbf{x}, \mathbf{y})\) as a function of \(T(\mathbf{x}, \mathbf{y})\), and note that \(\Lambda(\mathbf{x}, \mathbf{y})\) is unimodal with respect to \(T(\mathbf{x}, \mathbf{y})\). Now conclude that \(\Lambda(\mathbf{x}, \mathbf{y})\) is small if and only if \(T(\mathbf{x}, \mathbf{y})\) is small or large. For (c), \(\sum_{i} X_{i}\) and \(\sum_{j} Y_{j}\) follow Gamma distributions; there is a relation between Gamma and Beta random variables).
\end{ex}

\begin{ex}
    Let \(X_{1}, \ldots, X_{n} \stackrel{\text { i.i.d. }}{\sim} \mathcal{N}\left(\mu_{1}, \sigma^{2}\right)\) and \(Y_{1}, \ldots, Y_{m} \stackrel{\text { i.i.d. }}{\sim} \mathcal{N}\left(\mu_{2}, \sigma^{2}\right)\) be two independent samples. We are interested in testing
    \[
        H_{0}: \mu_{1}=\mu_{2} \quad \text { vs } \quad H_{0}: \mu_{1} \neq \mu_{2} .
    \]
    \begin{enumerate}[(a)]
        \item Derive the GLR test. Show that it can be based on the statistic
        \[
            T(\mathbf{x}, \mathbf{y})=\frac{\bar{X}-\bar{Y}}{\sqrt{S_{p}^{2}\left(\frac{1}{n}+\frac{1}{m}\right)}}, \quad S_{p}^{2}=\frac{1}{n+m-2}\left(\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}+\sum_{j=1}^{m}\left(Y_{j}-\bar{Y}\right)^{2}\right). 
        \]
        \item Show that
        \[
            T(\mathbf{x}, \mathbf{y}) \sim t_{n+m-2} \quad \text { under } H_{0} .
        \]
    \end{enumerate}
    In other words, the GLR test is equivalent to the so-called two-sample \(t\) test (remark: the case where the variances from the two samples are potentially distinct is called the Behrens-Fisher problem, and it is surprisingly intricate). 
\end{ex}

\begin{ex}
    Consider the hypotheses
    \(H_{0}: f_{0}(\mathbf{x})\) is true vs \(H_{1}: f_{1}(\mathbf{x})\) is true,
    where \(f_{j}(\mathbf{x}), j=0,1\), are generalized densities with respect to some \(\sigma\)-finite measure \(G(d \mathbf{x})\) and satisfying the identifiability condition \(G\left(\left\{\mathbf{x}: f_{1}(\mathbf{x}) \neq f_{0}(\mathbf{x})\right\}\right)>0\) (cf. the NeymanPearson lemma). Assume the distribution of the likelihood ratio statistic \(\frac{f_{1}(\mathbf{X})}{f_{0}(\mathbf{X})}\) is continuous under \(H_{0}\) (i.e., the c.d.f. of \(\frac{f_{1}(\mathbf{X})}{f_{0}(\mathbf{X})}\) is continuous). Show that the rejection regions of the MP test are nested, i.e.,
    \[
        \alpha<\alpha^{\prime} \Rightarrow R_{\alpha} \subseteq R_{\alpha^{\prime}}. 
    \]
\end{ex}

\begin{ex}
    Consider the hypotheses
    \[
        H_{0}: \theta \in \Theta_{0} \quad \text { vs } \quad H_{1}: \theta \in \Theta_{1}, 
    \]
    and assume the rejections regions \(\left\{R_{\alpha}\right\}_{\alpha \in(0,1)}\) for a given (family of) tests are nested. 
    \begin{enumerate}[(a)]
        \item if \(\sup _{\theta \in \Theta_{0}} \mathbb{P}_{\theta}\left(X \in R_{\alpha}\right) \leq \alpha, \alpha \in(0,1)\), show that
        \[
            \mathbb{P}_{\theta}(\widehat{p} \leq u) \leq u, \quad 0 \leq u \leq 1, \quad \theta \in \Theta_{0}. 
        \]
        \item if \(\mathbb{P}_{\theta}\left(X \in R_{\alpha}\right)=\alpha, \alpha \in(0,1)\), show that
        \[
            \mathbb{P}_{\theta}(\widehat{p} \leq u)=u, \quad 0 \leq u \leq 1, \quad \theta \in \Theta_{0}. 
        \]
    \end{enumerate}
    {\bfseries Suggestion: }

    (a) note that \(\{\widehat{p} \leq u\} \subseteq\left\{X \in R_{v}\right\}, v>u\), by the assumption of "nestedness" (interpretation: if \(\widehat{p}\) is small with respect to \(u\), then \(X\) must be in all rejection regions at a level \(v>u\). Observe that this is not necessarily true for \(\left\{X \in R_{u}\right\}\). Consequently,
    \[
    \mathbb{P}_{\theta}(\widehat{p} \leq u) \leq \mathbb{P}_{\theta}\left(X \in R_{v}\right) \leq v
    \]
    and now take \(v \downarrow u\) and \(\sup _{\theta \in \Theta_{0}}\). 

    (b) note that \(\left\{X \in R_{v}\right\} \subseteq\{\widehat{p} \leq u\}\). Now take \(\mathbb{P}_{\theta}\) and use (a) to conclude that equality holds. 
\end{ex}


\begin{ex}
    \rmk{3} in this problem, we build a MP test with non-nested rejection regions. 

    Consider the following two distributions. 
    \begin{center}
        \begin{tabular}{cccc}
            \hline\(x\) & 1 & 2 & 3 \\
            \hline\(f_{0}(x)\) & \(0.85\) & \(0.10\) & \(0.05\) \\
            \(f_{1}(x)\) & \(0.70\) & \(0.20\) & \(0.10\) \\
            \hline\(f_{1}(x) / f_{0}(x)\) & \(0.82\) & 2 & 2 \\
            \hline
        \end{tabular}
    \end{center}
    \begin{enumerate}[(a)]
        \item Just to fix ideas, use the argument for existence in the Neyman-Pearson lemma to construct a randomized test \(\phi\) of size \(\alpha=0.10\) (hint: choose \(k=2\) ). 
        \item Now define the test
        \[
            \psi(x)=\left\{\begin{array}{lc}
            1, & \text { if } x=2 \\
            0, & \text { if } x=1,3
            \end{array}\right.
        \]
        Conclude that \(\psi\) has size \(\alpha=0.10\). Use the Neyman-Pearson lemma to conclude that \(\psi\) is MP (n.b.: as expected, \(\psi\) and \(\phi\) do agree in the region \(\left\{x: f_{1}(x) \neq 2 f_{0}(x)\right\}\). 
        \item We now redefine \(\psi(x)\) at a different size, namely, \(\alpha=0.05\) (n.b.: rejection regions do change as a function of \(\alpha\) ). Let
        \[
            \psi(x)=\left\{\begin{array}{lc}
            1, & \text { if } x=3 \\
            0, & \text { if } x=1,2
            \end{array}\right.
        \]
        Conclude that \(\psi\) has size \(\alpha=0.05\), indeed. Use the Neyman-Pearson lemma to conclude that \(\psi\) is MP. 
        \item Conclude that \(R_{0.05} \nsubseteq R_{0.10}\). 
    \end{enumerate}
\end{ex}








