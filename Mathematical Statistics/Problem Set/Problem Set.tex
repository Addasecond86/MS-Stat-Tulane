\documentclass[12pt]{article}

\newcommand{\CN}{Mathematical Statistics}
\newcommand{\Ti}{Problem Set}
\newcommand{\Pf}{Dr. Didier}
\newcommand{\FN}{Zehao}
\newcommand{\LN}{Wang}



\input{settings.tex}


\begin{document}
% \maketitle
\begin{titlepage}
    \begin{center}    
    \includegraphics[width=0.6\textwidth]{Tulane.png}\\[1cm]    
    
    \textsc{\Huge \CN}\\[0.5cm]
    \textsc{\large \Pf}\\[1.0cm]
    
    \textsc{\LARGE \Ti}\\[0.5cm]
    \textsc{\large \LN, \FN}\\
    {Master student in Statistics of Math Dept.}
    
    % Author and supervisor
    
    \vfill
    
    % Bottom of the page
    {\Large \emph{\today}}
    
    \end{center}
\end{titlepage}
    
    \section{}

    \begin{exercise}
        Let \(X_{1}, \cdots, X_{n}\) be a random sample from the a.c. distribution
        \[
        f(x \mid \theta)=\theta x^{\theta-1} 1_{\{0 \leqslant x \leqslant 1\}}, \quad 0<\theta<\infty .
        \]
        Find the method of moments estimator of \(\theta\).
    \end{exercise}

    \begin{solution}
        For the sample \(\{X_i\}_{i=1}^n\), 
        \[
            \begin{aligned}
                \bar{X}=\mu_1=E(X_i)&=\int_0^1xf(x)\der x=\int_0^1\theta x^\theta\der x\\
                &=\frac{\theta}{\theta+1}x^{\theta+1}\big|_0^1=\frac{\theta}{\theta+1}. 
            \end{aligned}
        \]
        So, the method of moments estimator for $\theta$ is 
        \[
            \hat{\mu}_1=\frac{1}{n}\sum_{i=1}^nX_i=\frac{\theta}{\theta+1}
        \]
        \[
            \hat{\theta}=\frac{\hat{\mu}_1}{1-\hat{\mu}_1}. 
        \]
    \end{solution}

    \begin{exercise}
        Let \(X_{1}, \cdots, X_{n} \stackrel{\text { i.i.d. }}{\sim} \operatorname{Geo}(p), 0<p<1\), i.e.,
        \[
        \mathbb{P}\left(X_{1}=k \mid p\right)=(1-p)^{k-1} p, \quad k \in \mathbb{N} .
        \]
        What is the method of moments estimator of \(p\) ?
    \end{exercise}

    \begin{solution}
        For the sample \(\{X_i\}_{i=1}^n\), 
        \[
            \begin{aligned}
                \bar{X}=\mu_1=E(X_i)&=\sum_{k=0}^\infty k\cdot (1-p)^{k-1}p=\frac{1-p}{p}
            \end{aligned}
        \]
        So, the method of moments estimator of $p$ is 
        \[
            \hat{\mu}_1=\frac{1}{n}\sum_{i=1}^nX_i=\frac{1-p}{p}
        \]
        \[
            \hat{p}=\frac{1}{\hat{\mu}_1+1}. 
        \]
    \end{solution}
        
    \begin{exercise}
        Assume \(X \sim B(n, p)\), where both \(n \in \mathbb{N}\) and \(p \in(0,1)\) are unknown. Given a random sample of \(N\) observations of \(X\), compute the method of moments estimator of \(n\) and \(p\).
    \end{exercise}

    \begin{solution}
        For the sample \(\{X_i\}_{i=1}^N\), 
        \[
            \bar{X}=\mu_1=E(X_i)=\sum_{k=0}^nk\cdot \binom{n}{k}p^k(1-p)^{n-k}=np, 
        \]
        \[
            Var(X_i)=E(X_i^2)-(E(X_i))^2=\mu_2-\mu^2=np(1-p), 
        \]
        Now, solve these two equations: 
        \[
            \left\{\begin{array}{l}
                \hat{\mu}_1=np\\
                \hat{\mu}_2-\hat{\mu}_1^2=np(1-p)
            \end{array}\right.
        \]
        We can get that
        \[
            \hat{p}=1-\frac{\hat{\mu}_2-\hat{\mu}_1^2}{\hat{\mu}_1},\qquad\hat{n}=\frac{\hat{\mu}_1^2}{\hat{\mu}_1-\hat{\mu}_2+\hat{\mu}_1^2}. 
        \]
    \end{solution}

    \begin{exercise}
        Let \(X_{1}, \cdots, X_{n}\) be a random sample from the density
        \[
        f(x \mid \theta)=\theta x^{-2}, \quad 0<\theta \leqslant x<\infty .
        \]
        What can you say about the existence of the method of moments estimator of \(\theta\) ?
    \end{exercise}

    \begin{solution}
        \[
            E(X_i)=\int_{\theta}^\infty x\cdot \theta x^{-2}\der x=\theta \ln x \big|_\theta^\infty=\theta(\ln\infty-\ln\theta)=\infty, 
        \]
        So, the method of moments estimator for \(\theta\) does not exist. 
    \end{solution}

    \begin{exercise}
        Show that each of the following is an exponential family and describe the natural parameter space \(\mathcal{E}\) of the associated canonical exponential family.
        \begin{itemize}
            \item[(a)] \(\{\Gamma(\alpha, \lambda)\}_{\alpha, \lambda>0}\), i.e.,
            \[
            f(x \mid \alpha, \lambda)=\frac{\lambda^{\alpha}}{\Gamma(\alpha)} x^{\alpha-1} e^{-\lambda x} 1_{\{x>0\}} ;
            \]
            \item[(b)] \(\{\operatorname{Beta}(a, b)\}_{a, b>0}\), i.e.,
            \[
            f(x \mid a, b)=\frac{\Gamma(a+b)}{\Gamma(a) \Gamma(b)} x^{a-1}(1-x)^{b-1} 1_{\{0<x<1\}} ;
            \]
            \item[(c)] \(\{\operatorname{Poi}(\lambda)\}_{\lambda>0}\);
            \item[(d)] \(\{\operatorname{NegBin}(r, p)\}_{0 \leqslant p \leqslant 1}(r\) being known), i.e.,
            \[
            f(x \mid p)=\left(\begin{array}{c}
            r+x-1 \\
            x
            \end{array}\right) p^{r}(1-p)^{x}, \quad x \in \mathbb{N} \cup\{0\} .
            \]
        \end{itemize}
    \end{exercise}

    \begin{solution}
        \begin{itemize}
            \item[(a)]
            \[
                \begin{aligned}
                    f(x|\alpha, \lambda)&=1_{\{x>0\}}\exp(\alpha\ln(\lambda)-\ln(\Gamma(\alpha))+(\alpha-1)\ln(x)-\lambda x)\\
                    &=1_{\{x>0\}}\exp((\alpha-1)\ln(x)-\lambda x+\alpha\ln(\lambda)-\ln(\Gamma(\alpha))), 
                \end{aligned}
            \]
            So, the nature parameter is $(\eta_1,\eta_2)=(\alpha-1, -\lambda)$. And the parameter space \(\mathcal{E}\) is \((-1,\infty)\times(-\infty,0)\). 
            \item[(b)] 
            \[
                \begin{aligned}
                    f(x|a,b)=1_{\{0<x<1\}}&\exp((a-1)\ln(x)+(b-1)\ln(1-x)\\&+\ln(\Gamma(a+b))-\ln(\Gamma(a))-\ln(\Gamma(b))), 
                \end{aligned}
            \] 
            So, the nature parameter is $(\eta_1,\eta_2)=(a-1, b-1)$. And the parameter space \(\mathcal{E}\) is \((-1,\infty)\times(-1,\infty)\). 
            \item[(c)] When \(k\in\mathbb{N}\), 
            \[
                f(x)=P(X=k)=\frac{e^{-\lambda}\lambda^k}{k!}
            \]
            \[
                \begin{aligned}
                    P(X=k)=&1/k!\cdot\exp(-\lambda+k\ln(\lambda)), 
                \end{aligned}
            \]
            So, the nature parameter is $(\eta)=(\ln(\lambda))$. And the parameter space \(\mathcal{E}\) is \((-\infty,\infty)\). 
            \item[(d)] When \(x\in\mathbb{N}\), 
            \[
                \begin{aligned}
                    f(x|p)&=\binom{r+x-1}{x}\exp\left(r\ln(p)+x\ln(1-p)\right), 
                \end{aligned}
            \]
            So, the nature parameter is $(\eta)=(\ln(1-p))$. And the parameter space \(\mathcal{E}\) is \((-\infty,0]\). 
        \end{itemize}
    \end{solution}

    \begin{exercise}
        In \(n\) independent trials with \(k+1\) possible outcomes, let the probability of the \(i\)-th outcome be \(p_{i}\) in each trial. If \(X_{i}\) denotes the number of trials resulting in outcome \(i, i=0,1, \ldots, k\), then the joint distribution of \(X\) is the multinomial distribution, i.e., 
        \[
            f(\mathbf{x} \mid \mathbf{p}):=\mathbb{P}\left(X_{0}=x_{0}, X_{1}=x_{1}, \ldots, X_{k}=x_{k} \mid \mathbf{p}\right)=\frac{n !}{x_{0} ! x_{1} ! \ldots x_{k} !} p_{0}^{x_{0}} p_{1}^{x_{1}} \ldots p_{k}^{x_{k}}. 
        \]
        \begin{itemize}
            \item[(a)] Verify that \(\{f(\mathbf{x} \mid \mathbf{p})\}_{\mathbf{p} \in \Pi_{k}}\) is an exponential family, where \(\Pi_{k}\) is the \(k\)-dimensional simplex in \(\mathbb{R}^{k+1}\). Determine \(\mathcal{E}\). 
            \item[(b)] Using results for canonical exponential families, show that
            \[
                \mathbb{E} X_{i}=n p_{i}, \quad \operatorname{Cov}\left(X_{j}, X_{i}\right)=\left\{\begin{array}{cc}
                n p_{j}\left(1-p_{j}\right), & k=j \\
                -n p_{j} p_{k}, & k \neq j
            \end{array}\right.
            \]
        \end{itemize}
    \end{exercise}

    \begin{solution}
        \begin{itemize}
            \item[(a)] We know for every \(0\leqslant p_i\leqslant 1\). 
            \[
                f(x|\mathbf{p})=\frac{n !}{x_{0} ! x_{1} ! \ldots x_{k} !} \exp\left(x_0\ln (p_0)+\cdots+x_k\ln (p_k)\right). 
            \]
            So, the nature parameter is $(\eta_0,\cdots,\eta_k)=(\ln(p_0),\cdots,\ln(p_k))$. And the parameter space \(\mathcal{E}\) is \((-\infty,0]\times\cdots\times(-\infty,0]\). 
            \item[(b)] 
        \end{itemize}
    \end{solution}

    \begin{exercise}
        The Inverse Gaussian density, \(I G(\mu, \lambda)\), is given by
        \[
            f(x \mid \mu, \lambda)=\left(\frac{\lambda}{2 \pi}\right)^{1 / 2} x^{-3 / 2} \exp \left\{\frac{-\lambda(x-\mu)^{2}}{2 \mu^{2} x}\right\}, \quad x>0, \quad \mu>0, \quad \lambda>0. 
        \]
        \begin{itemize}
            \item[(a)] Show that this is an exponential family generated by \(T(X)=-\frac{1}{2}\left(X, X^{-1}\right)^{T}\) and \(h(x)=\) \((2 \pi)^{-1 / 2} x^{-3 / 2}\). 
            \item[(b)] Show that the canonical parameters \(\eta_{1}, \eta_{2}\) are given by \(\eta_{1}=\mu^{-2} \lambda, \eta_{2}=\lambda\), and that \(A\left(\eta_{1}, \eta_{2}\right)=-\left(\frac{1}{2} \log \left(\eta_{2}\right)+\sqrt{\eta_{1} \eta_{2}}\right), \mathcal{E}=[0, \infty) \times(0, \infty)\). 
            \item[(c)] Find the moment generating function of \(T\) and show that \(\mathbb{E}(X)=\mu, \operatorname{Var}(X)=\mu^{3} \lambda^{-1}\), \(\mathbb{E}\left(X^{-1}\right)=\mu^{-1}+\lambda^{-1}, \operatorname{Var}\left(X^{-1}\right)=(\lambda \mu)^{-1}+2 \lambda^{-2} .\)
        \end{itemize}
    \end{exercise}

    \begin{solution}
        \begin{itemize}
            \item[(a)] \[
                \begin{aligned}
                    f(x|\mu,\lambda)&=(2 \pi)^{-1 / 2} x^{-3 / 2}\exp\left\{-\lambda(x^2-2\mu x+\mu^2)\cdot(2\mu^2x)^{-1}+\frac{1}{2}\ln(\lambda)\right\}\\
                    &=(2 \pi)^{-1 / 2} x^{-3 / 2}\exp\left\{\frac{-\lambda}{2\mu^2}x + \frac{\lambda}{\mu}-\frac{\lambda}{2}x^{-1}+\frac{1}{2}\ln(\lambda)\right\}
                \end{aligned}
            \]
            So, $T(X)=-\frac{1}{2}(X,X^{-1})$, \(h(x)=\) \((2 \pi)^{-1 / 2} x^{-3 / 2}\). 
            \item[(b)] $B((\lambda, \mu))=-(1/2\ln(\lambda)+\lambda/\mu)$, and $\eta=(\eta_1,\eta_2)=(\lambda/\mu^2, \lambda)$. So, 
            \[
                A(\eta)=-(1/2\ln(\eta_2)+(\eta_1\eta_2)^{1/2}). 
            \]
            \item[(c)] 
        \end{itemize}
    \end{solution}

    \begin{exercise}
        For each of the following families: \((i)\) verify that it is an exponential family; \((i i)\) describe the curve in which the parameter vector \(\theta\) lies (with respect to the natural parametrization). 
        \begin{itemize}
            \item[(a)] \(\left\{\mathcal{N}\left(\theta, a \theta^{2}\right)\right\}_{\theta \in \mathbb{R}}, a\) is known;
            \item[(b)] \(\{\Gamma(\alpha, \alpha)\}_{\alpha>0} ;\)
            \item[(c)] \(\{f(x \mid \theta)\}_{\theta \in \mathbb{R}}\), where \(f(x \mid \theta)=C \exp \left\{-(x-\theta)^{4}\right\}, x \in \mathbb{R}\), and \(C\) is a normalizing constant.
        \end{itemize}
    \end{exercise}

    \begin{exercise}
        (In this problem, we generalize the proposition shown in class) Let \(\{f(\mathbf{x} \mid \theta)\}_{\theta \in \Theta}\) be a \(k\) parameter exponential family generated by \((T, h)\). Show that the distribution for \(T(\mathbf{X})\) is also a \(k\)-parameter exponential family. Hint: let \(G_{*}(d \mathbf{x})=h(\mathbf{x}) G(d \mathbf{x})\) and define the induced probability measure
        \[
            \mathbb{P}_{T}(B):=\mathbb{P}(T(\mathbf{X}) \in B)=\int_{\mathbb{R}^{n}} 1_{B}(T(\mathbf{x})) \exp \{\langle W(\theta), T(\mathbf{x})\rangle-B(\theta)\} G_{*}(d \mathbf{x}), 
        \]
        for \(B \in \mathcal{B}\left(\mathbb{R}^{k}\right)\). Now recall the following fundamental result on changes of measure, established in the course Probability theory \(I\). 

        {\bfseries Theorem. (Meerschaert and Scheffler (2001), \(p .4)\)} If \(\mu(d x)\) is a measure on \(\mathcal{B}\left(\mathbb{R}^{d}\right)\) and if \(T: \mathbb{R}^{d} \rightarrow \mathbb{R}^{m}, f: \mathbb{R}^{m} \rightarrow \mathbb{R}^{n}\) are Borel measurable, then
        \[
            \int_{\mathbb{R}^{d}} f(T(\boldsymbol{x})) \mu(d \boldsymbol{x})=\int_{\mathbb{R}^{m}} f(\boldsymbol{y})(T \mu)(d \boldsymbol{y}),
        \]
        where we define the measure \((T \mu)(B)=\mu\left(T^{-1}(B)\right), B \in \mathcal{B}\left(\mathbb{R}^{m}\right)\). 
    \end{exercise}




    
    % \begin{proof}
        
    % \end{proof}
    % \printbibliography
\end{document}
