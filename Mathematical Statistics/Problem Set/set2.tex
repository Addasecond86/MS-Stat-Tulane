\section{Problem Set \#2}

\define{1} Let \(f(\cdot)\) be a density. 
\begin{itemize}
    \item The class \(\left\{f(\cdot-\mu\}_{\mu \in \mathbb{R}}\right.\) is called the location family with standard density \(f(\cdot)\), and \(\mu\) is called the location parameter for the family. 
    \item The class \(\{(1 / \sigma) f(\cdot / \sigma)\}_{\sigma \in(0, \infty)}\) is called the scale family with standard density \(f(\cdot)\), and \(\sigma\) is called the scale parameter for the family. 
    \item By extension, the class \(\{(1 / \sigma) f((\cdot-\mu) / \sigma)\}_{(\mu, \sigma) \in \mathbb{R} \times(0, \infty)}\) is called the location-scale family with standard density \(f(\cdot)\).
\end{itemize}

\begin{exercise}
    \begin{enumerate}[(a)]
        \item Let \(f(\cdot)\) be a density, let \((\mu, \sigma) \in \mathbb{R} \times(0, \infty)\). Show that \(X\) is a random variable with density
        \[
        \frac{1}{\sigma} f\left(\frac{\cdot-\mu}{\sigma}\right)
        \]
        if and only if there exists a random variable \(Z\) with density \(f(\cdot)\) such that \(X=\sigma Z+\mu\).
        \item Let \(X \sim\) Cauchy \((0,1)\), and recall that its density is given by
        \[
        f(x)=\frac{1}{\pi} \frac{1}{1+x^{2}}, \quad x \in \mathbb{R} .
        \]
        Consider the location-scale family generated by \(f\). We know that \(\mathbb{E}|X|=\infty\), and thus the parameters \(\mu\) and \(\sigma^{2}\) cannot be interpreted as the mean and the variance of \(X\). Notwithstanding this, show that
        \begin{enumerate}[(i)]
            \item \(\mathbb{P}(X \geq \mu)=\mathbb{P}(X \leq \mu)=\frac{1}{2} ;\)
            \item \(\mathbb{P}(X \geq \mu+\sigma)=\mathbb{P}(X \leq \mu-\sigma)=\frac{1}{4}\). 
        \end{enumerate}  In other words, \(\mu-\sigma, \mu\) and \(\mu+\sigma\) are the first, second and third quartiles of \(f\) (hint: consider first the case \(\mu=0\) and \(\sigma=1\)). 
        \item We say a family of c.d.f.s \(\{F(x \mid \theta)\}_{\theta \in \Theta}\) is stochastically increasing in \(\theta\) when \[\theta_{1}>\theta_{2}\Rightarrow F\left(x \mid \theta_{1}\right)\text{ is stochastically greater than }F\left(x \mid \theta_{2}\right), \]i.e., \(F\left(x \mid \theta_{1}\right) \leq F\left(x \mid \theta_{2}\right), x \in \mathbb{R}\), and \(F\left(x_{0} \mid \theta_{1}\right)<F\left(x_{0} \mid \theta_{2}\right)\) for some \(x_{0}\). Show that 
        \begin{enumerate}[(i)]
            \item a location family is stochastically increasing in its location parameter; 
            \item a scale family is stochastically increasing in its scale parameter if its support is \([0, \infty)\).
        \end{enumerate}
    \end{enumerate}
\end{exercise}

\begin{solution}
    \begin{enumerate}
        \item
        $\Rightarrow:$ If $X\sim\frac{1}{\sigma}f\left(\frac{\cdot-\mu}{\sigma}\right)$, Let $Z=\frac{X-\mu}{\sigma}$, 
        \begin{align*}
            P(Z\leqslant z)&=P\left(\frac{X-\mu}{\sigma}\leqslant z\right)\\
            &=P\left(X\leqslant\sigma z+\mu\right). 
        \end{align*}
        \[
            F_Z(z)=F_X(\sigma z+\mu), 
        \]
        \begin{align*}
            f_Z(z)&=\sigma f_X(\sigma z+\mu)\\
            &=\sigma \frac{1}{\sigma}f_X\left(\frac{\sigma z +\mu-\mu}{\sigma}\right)\\
            &=f_X(z)
        \end{align*}
        Which means $Z$ has density $f(\cdot)$. 

        $\Leftarrow$: If $Z\sim f(z)$, $X=\sigma Z+\mu$, 
        \begin{align*}
            P(X\leqslant x)&=P(\sigma Z+\mu\leqslant x)\\
            &=P(Z\leqslant\frac{x-\mu}{\sigma})\\
            &=F_Z\left(\frac{x-\mu}{\sigma}\right), 
        \end{align*}
        So, \[
            f_X(x)=\frac{1}{\sigma}f_Z\left(\frac{x-\mu}{\sigma}\right). 
        \]
        \item \begin{enumerate}[(i)]
            \item Because for a density function $f(\cdot)$, $\int f(x) \der x=1$, and Cauchy distribution is symmetric about the y-axis. So, 
            \[
                P(X\leqslant0)=P(X\geqslant0)=1/2. 
            \]
            \item \[
                P(X\leqslant1)=P(X\geqslant1)=\int_1^\infty\frac{1}{\pi}\frac{1}{1+x^2}\der x=\frac{1}{\pi}\arctan(x)\big|_1^\infty=1/4. 
            \]
        \end{enumerate}
        \item \begin{enumerate}[(i)]
            \item For a location family: $F(x|\mu)=F(x-\mu)$. If $\mu_1>\mu_2$, then $x-\mu_1<x-\mu_2$, 
            \[
                F(x|\mu_1)<F(x|\mu_2). 
            \]
            So, it is stochastically increasing. 
            \item For a location family: $F(x|\sigma)=1/\sigma F(x/\sigma)$. If $\sigma_1>\sigma_2$, then $1/\sigma_1<1/\sigma_2$, 
            \[
                F(x|\sigma_1)<F(x|\sigma_2). 
            \]
            So, it is stochastically increasing, too. 
        \end{enumerate}
    \end{enumerate}
\end{solution}

\begin{exercise}
    We saw the following result in class (an adaptation of Theorem 1.6.4 in Bickel and Doksum \((2002)\), p. 60\()\).
    
    \thm{1} Suppose that \(\{f(\boldsymbol{x} \mid \eta)\}_{\eta \in \mathcal{E}}\) is a canonical exponential family generated by \(\left(T_{k \times 1}, h\right)\), where \(\mathcal{E} \subseteq \mathbb{R}^{k}, \mathcal{E} \neq \emptyset\), is open. Then, the following are equivalent.
    \begin{enumerate}[(i)]
        \item For any \(\eta \in \mathcal{E}\), the random variables \(1, T_{1}(\boldsymbol{X}), \ldots, T_{k}(\boldsymbol{X})\) are not a.s. linearly dependent, i.e.
        \[
            \left(\mathbf{a}, a_{k+1}\right) \equiv\left(a_{1}, \ldots, a_{k}, a_{k+1}\right) \neq \mathbf{0} \Rightarrow \mathbb{P}_{\eta}\left(\langle\mathbf{a}, T(\boldsymbol{X})\rangle=a_{k+1}\right)<1, 
        \]
        where \(T(\boldsymbol{X}):=\left(T_{1}(\boldsymbol{X}), \ldots, T_{k}(\boldsymbol{X})\right)^T\); 
        \item \(\eta \in \mathcal{E}\) is identifiable;
        \item for any \(\eta \in \mathcal{E}, \operatorname{Var}_{\eta}(T)\) is symmetric positive definite;
        \item \(\mathcal{E} \ni \eta \mapsto A^{\prime}(\eta)\) is an injective mapping;
        \item \(\mathcal{E} \ni \eta \mapsto A(\eta)\) is a strictly convex mapping. 
    \end{enumerate}
    We showed in class that \((i) \Leftrightarrow(i i) \Leftrightarrow(i i i)\). Finish the theorem.
    
    \rmk{1} Notice that, for a generic function \(f, f\) being strictly convex does not imply that \(f^{\prime \prime}(x)\) is a positive definite matrix.

    \emph{\bfseries Suggested route:} \((v) \Rightarrow(i v),(i i i) \Rightarrow(v),(i v) \Rightarrow(i i)\). The latter two claims are simpler than the former. So, for \((v) \Rightarrow(i v)\), consider the following definition and propositions. 

    \define{2} Let \(C\) be a convex set, and consider \(F: C \rightarrow \mathbb{R}^{k}\). We say \(F\) is monotone on \(C\) if
    \[
    \left\langle F(x)-F\left(x^{\prime}\right), x-x^{\prime}\right\rangle \geq 0, \quad x \neq x^{\prime},
    \]
    and strictly monotone if strict inequality holds. Now prove the next two propositions. 

    \prop{1} Let \(C\) be a convex set, and consider \(F: C \rightarrow \mathbb{R}^{k}\). If \(F\) is strictly monotone on \(C\), then \(F\) is injective.

    \prop{2} Let \(C\) be an open, convex set, and consider \(f: C \rightarrow \mathbb{R}\). If \(f\) is strictly convex and differentiable on \(C\), then \(f^{\prime}\) is strictly monotone.

    On a related note, it can be shown that the converse to Proposition \(2.2\) is also true.
\end{exercise}

\begin{exercise}
    Let \(X \sim \mathcal{N}\left(\mu, \sigma^{2}\right)\). Consider any \(g \in C^{1}(\mathbb{R})\) such that \(\mathbb{E}_{\eta}\left|g^{\prime}(X)\right|<\infty\). 
    \begin{enumerate}[(a)]
        \item (Stein's identity) Prove Stein's identity, i.e., that
        \[
        \mathbb{E}_{\eta}\left(\left(\frac{h^{\prime}(X)}{h(X)}+\sum_{i=1}^{k} \eta_{i} T_{i}^{\prime}(X)\right) g(X)\right)=-\mathbb{E}_{\eta} g^{\prime}(X)
        \]
        in canonical parametrization. 
        
        Hint: consider first the case \(\mu=0\) and \(\sigma^{2}=1\). Let \(\phi(\cdot)\) be the density function. Now use the fact that \(\phi^{\prime}(z)=-z \phi(z)\) to rewrite \(\phi(x)\) as an integral from \(x\) to \(\infty\) and as another integral from \(-\infty\) to \(x\) in the expression
        \[
            -\mathbb{E} g^{\prime}(X)=-\mathbb{E} g^{\prime}(X)\left(1_{[0, \infty)}(X)+1_{(-\infty, 0)}(X)\right). 
        \]
        \item By choosing polynomial \(g(\cdot)\) functions, use Stein's identity to calculate the third and fourth moments of the \(\mathcal{N}\left(\mu, \sigma^{2}\right)\) distribution.
        
        \rmk{2} Stein's identity is one aspect of the so-named Stein's method for establishing limits in distribution and their rates of convergence.
    \end{enumerate}
\end{exercise}

\begin{exercise}
    Prove that any (measurable) injective function of a minimal sufficient statistic is a minimal sufficient statistic. 
\end{exercise}

\begin{exercise}
    Prove that, for any two statistics \(T, T^{*}\),
    \[
    T(\mathbf{X})=T(\mathbf{Y}) \Leftrightarrow T^{*}(\mathbf{X})=T^{*}(\mathbf{Y}) \quad \mathcal{P}-a.s.
    \]
    if and only if there are functions \(f\) and \(g\) such that
    \[
    T(\mathbf{X})=f\left[T^{*}(\mathbf{X})\right], \quad T^{*}(\mathbf{X})=g[T(\mathbf{X})] \quad \mathcal{P}-a.s.
    \]
\end{exercise}

\begin{exercise}
    Establish the following claims. 
    \begin{enumerate}[(a)]
        \item Let \(T\) and \(T^{*}\) be two statistics. Then, there are measurable functions \(f, g\) such that 
        \[
            T=f\left(T^{*}\right)\text{ and }T^{*}=g(T)
        \]
        if and only if \(\sigma(T)=\sigma\left(T^{*}\right)\)
        (i.e., the \(\sigma\)-algebras generated by \(T\) and \(T^{*}\) are identical). 
        
        Hint: recall, from probability theory, that if \(\mathbf{X}\) and \(\mathbf{Y}\) are two random vectors such that \(\mathbf{Y}\) is \(\sigma(\mathbf{X})\)-measurable, then there exists a measurable function \(\varphi\) such that \(\mathbf{Y}=\varphi(\mathbf{X})\). 
        \item Conclude that if we drop the condition " \(\mathcal{P}\)-a.s." in the definition of equivalent statistics, the slightly stronger notion of equivalence we obtain is identical to the equality of the \(\sigma\)-algebras generated by the statistics in question. 
    \end{enumerate}
\end{exercise}

\begin{exercise}
    Let \(\mathcal{P}=\{f(\mathbf{x} \mid \theta)\}_{\theta \in \Theta}\) be a parametric family. 
    \begin{enumerate}[(a)]
        \item Prove that \(\mathbf{X}\) is sufficient for \(\theta\). 
        \item In addition, suppose \(\mathcal{P}\) corresponds to an i.i.d. sample. Prove that \(\left(X_{(1)}, \ldots, X_{(n)}\right)^T\) is sufficient for \(\theta\). 
    \end{enumerate}
\end{exercise}

\begin{exercise}
    Let \(f\) be a positive integrable function over \((0, \infty)\), and let \(p(x \mid \theta)\) be the probability density function over \((0, \theta)\) the density defined by
    \[
        p(x \mid \theta)=\left\{\begin{array}{cl}
        c(\theta) f(x), & 0<x<\theta \\
        0, & \text { otherwise }
        \end{array}\right.
    \]
    If \(X_{1}, \ldots, X_{n}\) are i.i.d. with density \(p(\cdot \mid \theta)\), show that \(X_{(n)}\) is sufficient for \(\theta\). 
\end{exercise}

\begin{exercise}
    Let \(f\) be a positive integrable function over \(\mathbb{R}\), and let \(p(x \mid \xi, \eta)\) be the probability density function defined by
    \[
        p(x \mid \xi, \eta)=\left\{\begin{array}{cl}
        c(\xi, \eta) f(x), & \xi<x<\eta \\
        0, & \text { otherwise }
        \end{array}\right.
    \]
    If \(X_{1}, \cdots, X_{n}\) are i.i.d. with density \(p(\cdot \mid \xi, \eta)\), show that \(\left(X_{(1)}, X_{(n)}\right)\) is sufficient for \((\xi, \eta)\).
\end{exercise}

\begin{exercise}
    \begin{enumerate}[(a)]
        \item Prove that, if \(\mathbf{x}=\left(x_{1}, \ldots, x_{n}\right)\) and \(\mathbf{y}=\left(y_{1}, \ldots, y_{n}\right)\) have the same elementary symmetric functions
        \[
            \sum_{i} x_{i}=\sum_{i} y_{i}, \quad \sum_{i \neq j} x_{i} x_{j}=\sum_{i \neq j} y_{i} y_{j}, \quad \ldots, \quad \Pi_{i} x_{i}=\Pi_{i} y_{i}
        \]
        then \(\mathbf{y}\) is a permutation of \(\mathbf{x}\). 
        \item Consider a family \(\mathcal{P}=\{f(\cdot \mid \theta)\}_{\theta \in \Theta}\), where \(f(\cdot \mid \theta)\) is an a.c. density for an i.i.d. sample for all \(\theta\), and set
        \[
            T(\mathbf{X})=\left(X_{(1)}, \ldots, X_{(n)}\right)^T .
        \]
        Now consider \(U=\left(U_{1}(X), \ldots, U_{n}(X)\right)^T\), where
        \[
            U_{1}(X)=\sum_{i} X_{i}, \quad U_{2}(X)=\sum_{i \neq j} X_{i} X_{j}, \quad \ldots, \quad U_{n}(X)=\Pi_{i} X_{i} .
        \]
        Conclude that \(U\) and \(T\) are equivalent. 
    \end{enumerate}
\end{exercise}

\begin{exercise}
    Show that the order statistics are minimal sufficient for the location family
    \[
        p(\mathbf{x} \mid \theta)=f\left(x_{1}-\theta\right) \ldots f\left(x_{n}-\theta\right)
    \]
    when
    \[
        f(x)=\frac{1}{\pi} \frac{1}{1+x^{2}},
    \]
    i.e., Cauchy \((0,1)\). 
\end{exercise}

\begin{exercise}
    Consider the curved exponential family \(\{n \text { i.i.d. } \mathcal{N}(\theta, \theta)\}_{\theta>0}\). Show that the sufficient statistic
    \[
        T(\mathbf{X})=\left(\sum_{i=1}^{n} X_{i}, \sum_{i=1}^{n} X_{i}^{2}\right)^T
    \]
    is not minimal. 
\end{exercise}

\begin{exercise}
    Let \(\{f(\mathbf{x} \mid \theta)\}_{\theta \in \mathbb{R}}\) the parametric family corresponding to i.i.d. random variables with joint density function
    \[
        \begin{aligned}
            f(\mathbf{x} \mid \theta)&=C \exp \left\{-\sum_{i=1}^{n}\left(x_{i}-\theta\right)^{4}\right\}\\
            &=C \exp \left(-n \theta^{4}\right) \exp \left\{4 \theta^{3} \sum_{i=1}^{n} x_{i}-6 \theta^{2} \sum_{i=1}^{n} x_{i}^{2}+4 \theta \sum_{i=1}^{n} x_{i}^{3}-\sum_{i=1}^{n} x_{i}^{4}\right\}
        \end{aligned}
    \]
    for some \(C>0\). Express the parameter space and show that
    \[
        T(\mathbf{X})=\left(\sum_{i=1}^{n} X_{i}, \sum_{i=1}^{n} X_{i}^{2}, \sum_{i=1}^{n} X_{i}^{3}\right)^T
    \]
    is a minimal sufficient statistic. 
\end{exercise}

\begin{exercise}
    Consider an i.i.d. Poisson family with parameter \(\lambda \in(0, \infty)\), i.e.,
    \[
        f(\mathbf{x} \mid \lambda)=e^{-n \lambda} e^{(\log \lambda) \sum_{i=1}^{n} x_{i}}\left(\prod_{i=1}^{n} x_{i} !\right)^{-1}
    \]
    \begin{enumerate}[(a)]
        \item Conclude that \(\{f(\mathbf{x} \mid \lambda)\}_{\lambda>0}\) is generated by \((T(\mathbf{x}), h(\mathbf{x}))=\left(\sum_{i=1}^{n} x_{i},\left(\prod_{i=1}^{n} x_{i} !\right)^{-1}\right)\), and that \(\mathcal{E}=\mathbb{R}\). Also conclude that \(T(\mathbf{X})\) is minimal sufficient for \(\lambda\). 
        \item Now fix \(\alpha \in \mathbb{R}\) and let \(T_{1}(\mathbf{X})=\alpha T(\mathbf{X}), T_{2}(\mathbf{X})=(1-\alpha) T(\mathbf{X})\). Conclude that \(\{f(\mathbf{x} \mid \lambda)\}_{\lambda>0}\) is generated by \(\left(\left(T_{1}(\mathbf{x}), T_{2}(\mathbf{x})\right) ;\left(\prod_{i=1}^{n} x_{i} !\right)^{-1}\right)\). 
        \item Show that the natural parameter space \(\mathcal{E}^{\prime}\) generated by \(\left(T_{1}(\mathbf{X}), T_{2}(\mathbf{X})\right)\) is \(\mathbb{R}^{2}\). Conclude that \(\left(T_{1}(\mathbf{X}), T_{2}(\mathbf{X})\right)\) is minimal sufficient for \(\lambda\). 
        \item Does (c) contradict the minimal sufficiency of \(T(\mathbf{X})\)? 
    \end{enumerate}
\end{exercise}


% \begin{solution}
%     \begin{enumerate}
%         \item A Table: 
        
%         \begin{mytable}[nofloat]{Table}
%             \small
%             \begin{tabular}{|l|lll|lll|}
%                 \hline
%                 Method      &                               & Gender                      &                   &                               & CIRSTTL                      &                   \\ \cline{2-7} 
%                             & \multicolumn{1}{l|}{Estimate} & \multicolumn{1}{l|}{SE}     & P-value           & \multicolumn{1}{l|}{Estimate} & \multicolumn{1}{l|}{SE}      & P-value           \\ \hline
%                 Pooled Data & \multicolumn{1}{l|}{-2.2414}  & \multicolumn{1}{l|}{0.3652} & \textless{}0.0001 & \multicolumn{1}{l|}{0.7239}   & \multicolumn{1}{l|}{0.0588}  & \textless{}0.0001 \\ \hline
%                 GEE(Ind)    & \multicolumn{1}{l|}{-2.2414}  & \multicolumn{1}{l|}{0.5793} & 0.0001            & \multicolumn{1}{l|}{0.7239}   & \multicolumn{1}{l|}{0.1017}  & \textless{}0.0001 \\ \hline
%                 GEE(Exch)   & \multicolumn{1}{l|}{-2.0592}  & \multicolumn{1}{l|}{0.5925} & 0.0005            & \multicolumn{1}{l|}{0.558}    & \multicolumn{1}{l|}{0.08676} & \textless{}0.0001 \\ \hline
%                 GEE(AR1)    & \multicolumn{1}{l|}{-2.3067}  & \multicolumn{1}{l|}{0.5796} & \textless{}0.0001 & \multicolumn{1}{l|}{0.6334}   & \multicolumn{1}{l|}{0.0911}  & \textless{}0.0001 \\ \hline
%                 \end{tabular}            
%         \end{mytable}
%     \end{enumerate}
    
% \end{solution}
