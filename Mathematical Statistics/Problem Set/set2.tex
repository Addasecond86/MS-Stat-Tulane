\section{Problem Set \#2}

\define{1} Let \(f(\cdot)\) be a density. 
\begin{itemize}
    \item The class \(\left\{f(\cdot-\mu\}_{\mu \in \mathbb{R}}\right.\) is called the location family with standard density \(f(\cdot)\), and \(\mu\) is called the location parameter for the family. 
    \item The class \(\{(1 / \sigma) f(\cdot / \sigma)\}_{\sigma \in(0, \infty)}\) is called the scale family with standard density \(f(\cdot)\), and \(\sigma\) is called the scale parameter for the family. 
    \item By extension, the class \(\{(1 / \sigma) f((\cdot-\mu) / \sigma)\}_{(\mu, \sigma) \in \mathbb{R} \times(0, \infty)}\) is called the location-scale family with standard density \(f(\cdot)\).
\end{itemize}

\begin{exercise}
    \begin{enumerate}[(a)]
        \item Let \(f(\cdot)\) be a density, let \((\mu, \sigma) \in \mathbb{R} \times(0, \infty)\). Show that \(X\) is a random variable with density
        \[
        \frac{1}{\sigma} f\left(\frac{\cdot-\mu}{\sigma}\right)
        \]
        if and only if there exists a random variable \(Z\) with density \(f(\cdot)\) such that \(X=\sigma Z+\mu\).
        \item Let \(X \sim\) Cauchy \((0,1)\), and recall that its density is given by
        \[
        f(x)=\frac{1}{\pi} \frac{1}{1+x^{2}}, \quad x \in \mathbb{R} .
        \]
        Consider the location-scale family generated by \(f\). We know that \(\mathbb{E}|X|=\infty\), and thus the parameters \(\mu\) and \(\sigma^{2}\) cannot be interpreted as the mean and the variance of \(X\). Notwithstanding this, show that
        \begin{enumerate}[(i)]
            \item \(\mathbb{P}(X \geq \mu)=\mathbb{P}(X \leq \mu)=\frac{1}{2} ;\)
            \item \(\mathbb{P}(X \geq \mu+\sigma)=\mathbb{P}(X \leq \mu-\sigma)=\frac{1}{4}\). 
        \end{enumerate}  In other words, \(\mu-\sigma, \mu\) and \(\mu+\sigma\) are the first, second and third quartiles of \(f\) (hint: consider first the case \(\mu=0\) and \(\sigma=1\)). 
        \item We say a family of c.d.f.s \(\{F(x \mid \theta)\}_{\theta \in \Theta}\) is stochastically increasing in \(\theta\) when \[\theta_{1}>\theta_{2}\Rightarrow F\left(x \mid \theta_{1}\right)\text{ is stochastically greater than }F\left(x \mid \theta_{2}\right), \]i.e., \(F\left(x \mid \theta_{1}\right) \leq F\left(x \mid \theta_{2}\right), x \in \mathbb{R}\), and \(F\left(x_{0} \mid \theta_{1}\right)<F\left(x_{0} \mid \theta_{2}\right)\) for some \(x_{0}\). Show that 
        \begin{enumerate}[(i)]
            \item a location family is stochastically increasing in its location parameter; 
            \item a scale family is stochastically increasing in its scale parameter if its support is \([0, \infty)\).
        \end{enumerate}
    \end{enumerate}
\end{exercise}

\begin{exercise}
    We saw the following result in class (an adaptation of Theorem 1.6.4 in Bickel and Doksum \((2002)\), p. 60\()\).
    
    \thm{1} Suppose that \(\{f(\boldsymbol{x} \mid \eta)\}_{\eta \in \mathcal{E}}\) is a canonical exponential family generated by \(\left(T_{k \times 1}, h\right)\), where \(\mathcal{E} \subseteq \mathbb{R}^{k}, \mathcal{E} \neq \emptyset\), is open. Then, the following are equivalent.
    \begin{enumerate}[(i)]
        \item For any \(\eta \in \mathcal{E}\), the random variables \(1, T_{1}(\boldsymbol{X}), \ldots, T_{k}(\boldsymbol{X})\) are not a.s. linearly dependent, i.e.
        \[
            \left(\mathbf{a}, a_{k+1}\right) \equiv\left(a_{1}, \ldots, a_{k}, a_{k+1}\right) \neq \mathbf{0} \Rightarrow \mathbb{P}_{\eta}\left(\langle\mathbf{a}, T(\boldsymbol{X})\rangle=a_{k+1}\right)<1, 
        \]
        where \(T(\boldsymbol{X}):=\left(T_{1}(\boldsymbol{X}), \ldots, T_{k}(\boldsymbol{X})\right)^T\); 
        \item \(\eta \in \mathcal{E}\) is identifiable;
        \item for any \(\eta \in \mathcal{E}, \operatorname{Var}_{\eta}(T)\) is symmetric positive definite;
        \item \(\mathcal{E} \ni \eta \mapsto A^{\prime}(\eta)\) is an injective mapping;
        \item \(\mathcal{E} \ni \eta \mapsto A(\eta)\) is a strictly convex mapping. 
    \end{enumerate}
    We showed in class that \((i) \Leftrightarrow(i i) \Leftrightarrow(i i i)\). Finish the theorem.
    
    \rmk{1} Notice that, for a generic function \(f, f\) being strictly convex does not imply that \(f^{\prime \prime}(x)\) is a positive definite matrix.

    \emph{\bfseries Suggested route:} \((v) \Rightarrow(i v),(i i i) \Rightarrow(v),(i v) \Rightarrow(i i)\). The latter two claims are simpler than the former. So, for \((v) \Rightarrow(i v)\), consider the following definition and propositions. 

    \define{2} Let \(C\) be a convex set, and consider \(F: C \rightarrow \mathbb{R}^{k}\). We say \(F\) is monotone on \(C\) if
    \[
    \left\langle F(x)-F\left(x^{\prime}\right), x-x^{\prime}\right\rangle \geq 0, \quad x \neq x^{\prime},
    \]
    and strictly monotone if strict inequality holds. Now prove the next two propositions. 

    \prop{1} Let \(C\) be a convex set, and consider \(F: C \rightarrow \mathbb{R}^{k}\). If \(F\) is strictly monotone on \(C\), then \(F\) is injective.

    \prop{2} Let \(C\) be an open, convex set, and consider \(f: C \rightarrow \mathbb{R}\). If \(f\) is strictly convex and differentiable on \(C\), then \(f^{\prime}\) is strictly monotone.

    On a related note, it can be shown that the converse to Proposition \(0.2\) is also true.
\end{exercise}

\begin{exercise}
    Let \(X \sim \mathcal{N}\left(\mu, \sigma^{2}\right)\). Consider any \(g \in C^{1}(\mathbb{R})\) such that \(\mathbb{E}_{\eta}\left|g^{\prime}(X)\right|<\infty\). 
    \begin{enumerate}[(a)]
        \item (Stein's identity) Prove Stein's identity, i.e., that
        \[
        \mathbb{E}_{\eta}\left(\left(\frac{h^{\prime}(X)}{h(X)}+\sum_{i=1}^{k} \eta_{i} T_{i}^{\prime}(X)\right) g(X)\right)=-\mathbb{E}_{\eta} g^{\prime}(X)
        \]
        in canonical parametrization. 
        
        Hint: consider first the case \(\mu=0\) and \(\sigma^{2}=1\). Let \(\phi(\cdot)\) be the density function. Now use the fact that \(\phi^{\prime}(z)=-z \phi(z)\) to rewrite \(\phi(x)\) as an integral from \(x\) to \(\infty\) and as another integral from \(-\infty\) to \(x\) in the expression
        \[
            -\mathbb{E} g^{\prime}(X)=-\mathbb{E} g^{\prime}(X)\left(1_{[0, \infty)}(X)+1_{(-\infty, 0)}(X)\right). 
        \]
        \item By choosing polynomial \(g(\cdot)\) functions, use Stein's identity to calculate the third and fourth moments of the \(\mathcal{N}\left(\mu, \sigma^{2}\right)\) distribution.
        
        \rmk{2} Stein's identity is one aspect of the so-named Stein's method for establishing limits in distribution and their rates of convergence.
    \end{enumerate}
\end{exercise}

\begin{exercise}
    Prove that any (measurable) injective function of a minimal sufficient statistic is a minimal sufficient statistic. 
\end{exercise}

\begin{exercise}
    Prove that, for any two statistics \(T, T^{*}\),
    \[
    T(\mathbf{X})=T(\mathbf{Y}) \Leftrightarrow T^{*}(\mathbf{X})=T^{*}(\mathbf{Y}) \quad \mathcal{P}-a.s.
    \]
    if and only if there are functions \(f\) and \(g\) such that
    \[
    T(\mathbf{X})=f\left[T^{*}(\mathbf{X})\right], \quad T^{*}(\mathbf{X})=g[T(\mathbf{X})] \quad \mathcal{P}-a.s.
    \]
\end{exercise}

\begin{exercise}
    Establish the following claims. 
    \begin{enumerate}[(a)]
        \item Let \(T\) and \(T^{*}\) be two statistics. Then, there are measurable functions \(f, g\) such that 
        \[
            T=f\left(T^{*}\right)\text{ and }T^{*}=g(T)
        \]
        if and only if \(\sigma(T)=\sigma\left(T^{*}\right)\)
        (i.e., the \(\sigma\)-algebras generated by \(T\) and \(T^{*}\) are identical). 
        
        Hint: recall, from probability theory, that if \(\mathbf{X}\) and \(\mathbf{Y}\) are two random vectors such that \(\mathbf{Y}\) is \(\sigma(\mathbf{X})\)-measurable, then there exists a measurable function \(\varphi\) such that \(\mathbf{Y}=\varphi(\mathbf{X})\). 
        \item Conclude that if we drop the condition " \(\mathcal{P}\)-a.s." in the definition of equivalent statistics, the slightly stronger notion of equivalence we obtain is identical to the equality of the \(\sigma\)-algebras generated by the statistics in question. 
    \end{enumerate}
\end{exercise}

\begin{exercise}
    Let \(\mathcal{P}=\{f(\mathbf{x} \mid \theta)\}_{\theta \in \Theta}\) be a parametric family. 
    \begin{enumerate}[(a)]
        \item Prove that \(\mathbf{X}\) is sufficient for \(\theta\). 
        \item In addition, suppose \(\mathcal{P}\) corresponds to an i.i.d. sample. Prove that \(\left(X_{(1)}, \ldots, X_{(n)}\right)^T\) is sufficient for \(\theta\). 
    \end{enumerate}
\end{exercise}

\begin{exercise}
    Let \(f\) be a positive integrable function over \((0, \infty)\), and let \(p(x \mid \theta)\) be the probability density function over \((0, \theta)\) the density defined by
    \[
        p(x \mid \theta)=\left\{\begin{array}{cl}
        c(\theta) f(x), & 0<x<\theta \\
        0, & \text { otherwise }
        \end{array}\right.
    \]
    If \(X_{1}, \ldots, X_{n}\) are i.i.d. with density \(p(\cdot \mid \theta)\), show that \(X_{(n)}\) is sufficient for \(\theta\). 
\end{exercise}

\begin{exercise}
    Let \(f\) be a positive integrable function over \(\mathbb{R}\), and let \(p(x \mid \xi, \eta)\) be the probability density function defined by
    \[
        p(x \mid \xi, \eta)=\left\{\begin{array}{cl}
        c(\xi, \eta) f(x), & \xi<x<\eta \\
        0, & \text { otherwise }
        \end{array}\right.
    \]
    If \(X_{1}, \cdots, X_{n}\) are i.i.d. with density \(p(\cdot \mid \xi, \eta)\), show that \(\left(X_{(1)}, X_{(n)}\right)\) is sufficient for \((\xi, \eta)\).
\end{exercise}

\begin{exercise}
    \begin{enumerate}[(a)]
        \item Prove that, if \(\mathbf{x}=\left(x_{1}, \ldots, x_{n}\right)\) and \(\mathbf{y}=\left(y_{1}, \ldots, y_{n}\right)\) have the same elementary symmetric functions
        \[
            \sum_{i} x_{i}=\sum_{i} y_{i}, \quad \sum_{i \neq j} x_{i} x_{j}=\sum_{i \neq j} y_{i} y_{j}, \quad \ldots, \quad \Pi_{i} x_{i}=\Pi_{i} y_{i}
        \]
        then \(\mathbf{y}\) is a permutation of \(\mathbf{x}\). 
        \item Consider a family \(\mathcal{P}=\{f(\cdot \mid \theta)\}_{\theta \in \Theta}\), where \(f(\cdot \mid \theta)\) is an a.c. density for an i.i.d. sample for all \(\theta\), and set
        \[
            T(\mathbf{X})=\left(X_{(1)}, \ldots, X_{(n)}\right)^T .
        \]
        Now consider \(U=\left(U_{1}(X), \ldots, U_{n}(X)\right)^T\), where
        \[
            U_{1}(X)=\sum_{i} X_{i}, \quad U_{2}(X)=\sum_{i \neq j} X_{i} X_{j}, \quad \ldots, \quad U_{n}(X)=\Pi_{i} X_{i} .
        \]
        Conclude that \(U\) and \(T\) are equivalent. 
    \end{enumerate}
\end{exercise}

\begin{exercise}
    Show that the order statistics are minimal sufficient for the location family
    \[
        p(\mathbf{x} \mid \theta)=f\left(x_{1}-\theta\right) \ldots f\left(x_{n}-\theta\right)
    \]
    when
    \[
        f(x)=\frac{1}{\pi} \frac{1}{1+x^{2}},
    \]
    i.e., Cauchy \((0,1)\). 
\end{exercise}

\begin{exercise}
    Consider the curved exponential family \(\{n \text { i.i.d. } \mathcal{N}(\theta, \theta)\}_{\theta>0}\). Show that the sufficient statistic
    \[
        T(\mathbf{X})=\left(\sum_{i=1}^{n} X_{i}, \sum_{i=1}^{n} X_{i}^{2}\right)^T
    \]
    is not minimal. 
\end{exercise}

\begin{exercise}
    Let \(\{f(\mathbf{x} \mid \theta)\}_{\theta \in \mathbb{R}}\) the parametric family corresponding to i.i.d. random variables with joint density function
    \[
        \begin{aligned}
            f(\mathbf{x} \mid \theta)&=C \exp \left\{-\sum_{i=1}^{n}\left(x_{i}-\theta\right)^{4}\right\}\\
            &=C \exp \left(-n \theta^{4}\right) \exp \left\{4 \theta^{3} \sum_{i=1}^{n} x_{i}-6 \theta^{2} \sum_{i=1}^{n} x_{i}^{2}+4 \theta \sum_{i=1}^{n} x_{i}^{3}-\sum_{i=1}^{n} x_{i}^{4}\right\}
        \end{aligned}
    \]
    for some \(C>0\). Express the parameter space and show that
    \[
        T(\mathbf{X})=\left(\sum_{i=1}^{n} X_{i}, \sum_{i=1}^{n} X_{i}^{2}, \sum_{i=1}^{n} X_{i}^{3}\right)^T
    \]
    is a minimal sufficient statistic. 
\end{exercise}

\begin{exercise}
    Consider an i.i.d. Poisson family with parameter \(\lambda \in(0, \infty)\), i.e.,
    \[
        f(\mathbf{x} \mid \lambda)=e^{-n \lambda} e^{(\log \lambda) \sum_{i=1}^{n} x_{i}}\left(\prod_{i=1}^{n} x_{i} !\right)^{-1}
    \]
    \begin{enumerate}[(a)]
        \item Conclude that \(\{f(\mathbf{x} \mid \lambda)\}_{\lambda>0}\) is generated by \((T(\mathbf{x}), h(\mathbf{x}))=\left(\sum_{i=1}^{n} x_{i},\left(\prod_{i=1}^{n} x_{i} !\right)^{-1}\right)\), and that \(\mathcal{E}=\mathbb{R}\). Also conclude that \(T(\mathbf{X})\) is minimal sufficient for \(\lambda\). 
        \item Now fix \(\alpha \in \mathbb{R}\) and let \(T_{1}(\mathbf{X})=\alpha T(\mathbf{X}), T_{2}(\mathbf{X})=(1-\alpha) T(\mathbf{X})\). Conclude that \(\{f(\mathbf{x} \mid \lambda)\}_{\lambda>0}\) is generated by \(\left(\left(T_{1}(\mathbf{x}), T_{2}(\mathbf{x})\right) ;\left(\prod_{i=1}^{n} x_{i} !\right)^{-1}\right)\). 
        \item Show that the natural parameter space \(\mathcal{E}^{\prime}\) generated by \(\left(T_{1}(\mathbf{X}), T_{2}(\mathbf{X})\right)\) is \(\mathbb{R}^{2}\). Conclude that \(\left(T_{1}(\mathbf{X}), T_{2}(\mathbf{X})\right)\) is minimal sufficient for \(\lambda\). 
        \item Does (c) contradict the minimal sufficiency of \(T(\mathbf{X})\)? 
    \end{enumerate}
\end{exercise}

