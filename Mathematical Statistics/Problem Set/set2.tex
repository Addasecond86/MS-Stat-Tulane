\section{Problem Set \#2}

\define{1} Let \(f(\cdot)\) be a density. 
\begin{itemize}
    \item The class \(\left\{f(\cdot-\mu\}_{\mu \in \mathbb{R}}\right.\) is called the location family with standard density \(f(\cdot)\), and \(\mu\) is called the location parameter for the family. 
    \item The class \(\{(1 / \sigma) f(\cdot / \sigma)\}_{\sigma \in(0, \infty)}\) is called the scale family with standard density \(f(\cdot)\), and \(\sigma\) is called the scale parameter for the family. 
    \item By extension, the class \(\{(1 / \sigma) f((\cdot-\mu) / \sigma)\}_{(\mu, \sigma) \in \mathbb{R} \times(0, \infty)}\) is called the location-scale family with standard density \(f(\cdot)\).
\end{itemize}

\begin{exercise}
    \begin{enumerate}[(a)]
        \item Let \(f(\cdot)\) be a density, let \((\mu, \sigma) \in \mathbb{R} \times(0, \infty)\). Show that \(X\) is a random variable with density
        \[
        \frac{1}{\sigma} f\left(\frac{\cdot-\mu}{\sigma}\right)
        \]
        if and only if there exists a random variable \(Z\) with density \(f(\cdot)\) such that \(X=\sigma Z+\mu\).
        \item Let \(X \sim\) Cauchy \((0,1)\), and recall that its density is given by
        \[
        f(x)=\frac{1}{\pi} \frac{1}{1+x^{2}}, \quad x \in \mathbb{R} .
        \]
        Consider the location-scale family generated by \(f\). We know that \(\mathbb{E}|X|=\infty\), and thus the parameters \(\mu\) and \(\sigma^{2}\) cannot be interpreted as the mean and the variance of \(X\). Notwithstanding this, show that
        \begin{enumerate}[(i)]
            \item \(\mathbb{P}(X \geq \mu)=\mathbb{P}(X \leq \mu)=\frac{1}{2} ;\)
            \item \(\mathbb{P}(X \geq \mu+\sigma)=\mathbb{P}(X \leq \mu-\sigma)=\frac{1}{4}\). 
        \end{enumerate}  In other words, \(\mu-\sigma, \mu\) and \(\mu+\sigma\) are the first, second and third quartiles of \(f\) (hint: consider first the case \(\mu=0\) and \(\sigma=1\)). 
        \item We say a family of c.d.f.s \(\{F(x \mid \theta)\}_{\theta \in \Theta}\) is stochastically increasing in \(\theta\) when \[\theta_{1}>\theta_{2}\Rightarrow F\left(x \mid \theta_{1}\right)\text{ is stochastically greater than }F\left(x \mid \theta_{2}\right), \]i.e., \(F\left(x \mid \theta_{1}\right) \leq F\left(x \mid \theta_{2}\right), x \in \mathbb{R}\), and \(F\left(x_{0} \mid \theta_{1}\right)<F\left(x_{0} \mid \theta_{2}\right)\) for some \(x_{0}\). Show that 
        \begin{enumerate}[(i)]
            \item a location family is stochastically increasing in its location parameter; 
            \item a scale family is stochastically increasing in its scale parameter if its support is \([0, \infty)\).
        \end{enumerate}
    \end{enumerate}
\end{exercise}

\begin{solution}
    \begin{enumerate}
        \item
        $\Rightarrow:$ If $X\sim\frac{1}{\sigma}f\left(\frac{\cdot-\mu}{\sigma}\right)$, Let $Z=\frac{X-\mu}{\sigma}$, 
        \begin{align*}
            P(Z\leqslant z)&=P\left(\frac{X-\mu}{\sigma}\leqslant z\right)\\
            &=P\left(X\leqslant\sigma z+\mu\right). 
        \end{align*}
        \[
            F_Z(z)=F_X(\sigma z+\mu), 
        \]
        \begin{align*}
            f_Z(z)&=\sigma f_X(\sigma z+\mu)\\
            &=\sigma \frac{1}{\sigma}f_X\left(\frac{\sigma z +\mu-\mu}{\sigma}\right)\\
            &=f_X(z)
        \end{align*}
        Which means $Z$ has density $f(\cdot)$. 

        $\Leftarrow$: If $Z\sim f(z)$, $X=\sigma Z+\mu$, 
        \begin{align*}
            P(X\leqslant x)&=P(\sigma Z+\mu\leqslant x)\\
            &=P(Z\leqslant\frac{x-\mu}{\sigma})\\
            &=F_Z\left(\frac{x-\mu}{\sigma}\right), 
        \end{align*}
        So, \[
            f_X(x)=\frac{1}{\sigma}f_Z\left(\frac{x-\mu}{\sigma}\right). 
        \]
        \item \begin{enumerate}[(i)]
            \item Because for a density function $f(\cdot)$, $\int f(x) \der x=1$, and Cauchy distribution is symmetric about the y-axis. So, 
            \[
                P(X\leqslant0)=P(X\geqslant0)=1/2. 
            \]
            \item \[
                P(X\leqslant1)=P(X\geqslant1)=\int_1^\infty\frac{1}{\pi}\frac{1}{1+x^2}\der x=\frac{1}{\pi}\arctan(x)\big|_1^\infty=1/4. 
            \]
        \end{enumerate}
        \item \begin{enumerate}[(i)]
            \item For a location family: $F(x|\mu)=F(x-\mu)$. If $\mu_1>\mu_2$, then $x-\mu_1<x-\mu_2$, 
            \[
                F(x|\mu_1)<F(x|\mu_2). 
            \]
            So, it is stochastically increasing. 
            \item For a location family: $F(x|\sigma)=1/\sigma F(x/\sigma)$. If $\sigma_1>\sigma_2$, then $1/\sigma_1<1/\sigma_2$, 
            \[
                F(x|\sigma_1)<F(x|\sigma_2). 
            \]
            So, it is stochastically increasing, too. 
        \end{enumerate}
    \end{enumerate}
\end{solution}

\begin{exercise}
    We saw the following result in class (an adaptation of Theorem 1.6.4 in Bickel and Doksum \((2002)\), p. 60\()\).
    
    \thm{1} Suppose that \(\{f(\boldsymbol{x} \mid \eta)\}_{\eta \in \mathcal{E}}\) is a canonical exponential family generated by \(\left(T_{k \times 1}, h\right)\), where \(\mathcal{E} \subseteq \mathbb{R}^{k}, \mathcal{E} \neq \emptyset\), is open. Then, the following are equivalent.
    \begin{enumerate}[(i)]
        \item For any \(\eta \in \mathcal{E}\), the random variables \(1, T_{1}(\boldsymbol{X}), \ldots, T_{k}(\boldsymbol{X})\) are not a.s. linearly dependent, i.e.
        \[
            \left(\mathbf{a}, a_{k+1}\right) \equiv\left(a_{1}, \ldots, a_{k}, a_{k+1}\right) \neq \mathbf{0} \Rightarrow \mathbb{P}_{\eta}\left(\langle\mathbf{a}, T(\boldsymbol{X})\rangle=a_{k+1}\right)<1, 
        \]
        where \(T(\boldsymbol{X}):=\left(T_{1}(\boldsymbol{X}), \ldots, T_{k}(\boldsymbol{X})\right)^T\); 
        \item \(\eta \in \mathcal{E}\) is identifiable;
        \item for any \(\eta \in \mathcal{E}, \operatorname{Var}_{\eta}(T)\) is symmetric positive definite;
        \item \(\mathcal{E} \ni \eta \mapsto A^{\prime}(\eta)\) is an injective mapping;
        \item \(\mathcal{E} \ni \eta \mapsto A(\eta)\) is a strictly convex mapping. 
    \end{enumerate}
    We showed in class that \((i) \Leftrightarrow(i i) \Leftrightarrow(i i i)\). Finish the theorem.
    
    \rmk{1} Notice that, for a generic function \(f, f\) being strictly convex does not imply that \(f^{\prime \prime}(x)\) is a positive definite matrix.

    \emph{\bfseries Suggested route:} \((v) \Rightarrow(i v),(i i i) \Rightarrow(v),(i v) \Rightarrow(i i)\). The latter two claims are simpler than the former. So, for \((v) \Rightarrow(i v)\), consider the following definition and propositions. 

    \define{2} Let \(C\) be a convex set, and consider \(F: C \rightarrow \mathbb{R}^{k}\). We say \(F\) is monotone on \(C\) if
    \[
    \left\langle F(x)-F\left(x^{\prime}\right), x-x^{\prime}\right\rangle \geq 0, \quad x \neq x^{\prime},
    \]
    and strictly monotone if strict inequality holds. Now prove the next two propositions. 

    \prop{1} Let \(C\) be a convex set, and consider \(F: C \rightarrow \mathbb{R}^{k}\). If \(F\) is strictly monotone on \(C\), then \(F\) is injective.

    \prop{2} Let \(C\) be an open, convex set, and consider \(f: C \rightarrow \mathbb{R}\). If \(f\) is strictly convex and differentiable on \(C\), then \(f^{\prime}\) is strictly monotone.

    On a related note, it can be shown that the converse to Proposition \(2.2\) is also true.
\end{exercise}

\begin{solution}
    \emph{First, prove propositions 2.1: }

    If $F$ is not injective, then $\exists\, x_1\neq x_2$, s.t. $F(x_1)=F(x_2)$. So, $F$ is not strictly monotone. And we get a contradiction, i.e. $F$ must be injective. 

    \emph{Propositions 2.2 is the definition of convex function. }

    \emph{$v\to iv$: }

    Because $\eta$ is open set, and $f:\,\eta\to A(\eta)$ is strictly convex, then $A'(\eta)$ is strictly monotone, and is an injective mapping. 

    \emph{$iii\to v$: }

    $Var_\eta(T)>0$, i.e. $A''(\eta)>0$, then $A(\eta)$ is strictly convex. 

    \emph{$iv\to ii$: }

    If $\exists\,\eta_1\neq\eta_2$, s.t. $f(x|\eta_1)=f(x|\eta_2)$. Then 
    \[
        A'(\eta_1)=E_{\eta_1}(T(X))=E_{\eta_2}(T(X))=A'(\eta_2). 
    \]
    So, we ge the contradiction. i.e. $\eta$ is identifiable. 
\end{solution}

\begin{exercise}
    Let \(X \sim \mathcal{N}\left(\mu, \sigma^{2}\right)\). Consider any \(g \in C^{1}(\mathbb{R})\) such that \(\mathbb{E}_{\eta}\left|g^{\prime}(X)\right|<\infty\). 
    \begin{enumerate}[(a)]
        \item (Stein's identity) Prove Stein's identity, i.e., that
        \[
        \mathbb{E}_{\eta}\left(\left(\frac{h^{\prime}(X)}{h(X)}+\sum_{i=1}^{k} \eta_{i} T_{i}^{\prime}(X)\right) g(X)\right)=-\mathbb{E}_{\eta} g^{\prime}(X)
        \]
        in canonical parametrization. 
        
        Hint: consider first the case \(\mu=0\) and \(\sigma^{2}=1\). Let \(\phi(\cdot)\) be the density function. Now use the fact that \(\phi^{\prime}(z)=-z \phi(z)\) to rewrite \(\phi(x)\) as an integral from \(x\) to \(\infty\) and as another integral from \(-\infty\) to \(x\) in the expression
        \[
            -\mathbb{E} g^{\prime}(X)=-\mathbb{E} g^{\prime}(X)\left(1_{[0, \infty)}(X)+1_{(-\infty, 0)}(X)\right). 
        \]
        \item By choosing polynomial \(g(\cdot)\) functions, use Stein's identity to calculate the third and fourth moments of the \(\mathcal{N}\left(\mu, \sigma^{2}\right)\) distribution.
        
        \rmk{2} Stein's identity is one aspect of the so-named Stein's method for establishing limits in distribution and their rates of convergence.
    \end{enumerate}
\end{exercise}

\begin{solution}
    \begin{enumerate}[(a)]
        \item First, $\mu=0$, $\sigma^2=1$. Then 
        \[
            \phi(z)=\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{z^2}{2}\right). 
        \]
        \[
            \phi'(z)=-z\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{z^2}{2}\right)=-z\phi(z). 
        \]
        Then 
        \[
            \begin{aligned}
                E[g'(Z)]&=\int_{-\infty}^\infty g'(z)\phi(z)\der z\\
                &=\int_{-\infty}^0 g'(z)\phi(z)\der z + \int_{0}^\infty g'(z)\phi(z)\der z\\
                &=\int_{-\infty}^0 g'(z)\phi(z)\der z + \int_{0}^\infty g'(z)\int_z^\infty -\phi'(t)\der t \der z\\
                &=\int_{-\infty}^0 g'(z)\phi(z)\der z + \int_{0}^\infty g'(z)\int_z^\infty t\phi(t)\der t \der z\\
                &=\int_{-\infty}^0 g'(z)\phi(z)\der z + \int_{0}^\infty\int_z^\infty g'(z) t\phi(t)\der t \der z\\
                &=\int_{-\infty}^0 g'(z)\phi(z)\der z + \int_{0}^\infty\int_0^t g'(z) t\phi(t)\der z \der t\\
                &=\int_{-\infty}^0 g'(z)\phi(z)\der z + \int_{0}^\infty (g(t)-g(0))t\phi(t)\der t\\
                &=\int_{-\infty}^0 (g(t)-g(0))t\phi(t)\der t + \int_{0}^\infty (g(t)-g(0))t\phi(t)\der t\\
                &=E[z(g(z)-g(0))]=E[zg(z)-zg(0)]=E[zg(z)]. 
            \end{aligned}
        \]
        Let $g(x)=\phi\left(\frac{x-\mu}{\sigma}\right)$, then
        \[
            E[g'(x)]=E\left[\frac{1}{\sigma}\phi'\left(\frac{x-\mu}{\sigma}\right)\right]=\frac{1}{\sigma}E\left[\frac{x-\mu}{\sigma}\phi\left(\frac{x-\mu}{\sigma}\right)\right]=\frac{1}{\sigma}E\left[\frac{x-\mu}{\sigma}g(x)\right]
        \]
        For normal distribution: 
        \[
            \begin{aligned}
            \phi(x)&=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)\\
            &=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{1}{2\sigma^2}x^2+\frac{\mu}{\sigma^2}x-\frac{\mu^2}{2\sigma^2}\right)
            \end{aligned}
        \]
        $h'(x)=0$, and $T'_1(x)=x$, $T'_2(x)=1$, $\eta=(-1/\sigma^2, \mu/\sigma^2)$. 
        \[
            \begin{aligned}
                &\quad E\left(\left(\frac{h^{\prime}(X)}{h(X)}+\sum_{i=1}^{k} \eta_{i} T_{i}^{\prime}(X)\right) g(X)\right)\\
                &=E\left(\left(-\frac{1}{\sigma^2}x+\frac{\mu}{\sigma^2}\right) g(X)\right)\\
                &=E\left(\frac{\mu-x}{\sigma^2} g(X)\right)\\
                &=-\frac{1}{\sigma} E\left(\frac{x-\mu}{\sigma} g(X)\right)\\
                &=-E(g'(x)). 
            \end{aligned}
        \]
        \item We can know that \[
            \frac{1}{\sigma} E\left(\frac{x-\mu}{\sigma} g(X)\right)=E(g'(x)), 
        \]
        then let $g(x)=x^2$, 
        \[
            E\left(\frac{x-\mu}{\sigma^2}x^2\right)=2\mu, \Rightarrow E(x^3)=3\mu\sigma^2+\mu^3. 
        \]
        Let $g(x)=x^3$, then
        \[
            E\left(\frac{x-\mu}{\sigma^2}x^3\right)=3(\sigma^2+\mu^2), \Rightarrow E(x^4)=6\mu^2\sigma^2+\mu^4+3\sigma^4. 
        \]
    \end{enumerate}
\end{solution}

\begin{exercise}
    Prove that any (measurable) injective function of a minimal sufficient statistic is a minimal sufficient statistic. 
\end{exercise}

\begin{solution}
    Let $T(X)$ be a minimal sufficient statistic, $T^*(X)=f(T(X))$, $g(T^*(X))=T(X)$. Because $T(X)$ is minimal, there is another sufficient statistic $G(X)$ and a measurable function $h$, s.t. 
    \[
        h(G(X))=T(X), 
    \]
    then
    \[
        (f\circ h)(G(X))=f(T(X))=T^*(X). 
    \]
    So, $T^*(X)$ is also minimal. 
\end{solution}

\begin{exercise}
    Prove that, for any two statistics \(T, T^{*}\),
    \[
    T(\mathbf{X})=T(\mathbf{Y}) \Leftrightarrow T^{*}(\mathbf{X})=T^{*}(\mathbf{Y}) \quad \mathcal{P}-a.s.
    \]
    if and only if there are functions \(f\) and \(g\) such that
    \[
    T(\mathbf{X})=f\left[T^{*}(\mathbf{X})\right], \quad T^{*}(\mathbf{X})=g[T(\mathbf{X})] \quad \mathcal{P}-a.s.
    \]
\end{exercise}

\begin{solution}
    $\Rightarrow: $

    If $T^*(X)=T^*(Y)$, then
    \[
        T^*(X)=T^*(Y)\rightarrow f(T^*(X))=f(T^*(Y))\rightarrow T(X)=T(Y). 
    \]
    $\Leftarrow: $

    If $T(X)=T(Y)$, then
    \[
        T(X)=T(Y)\rightarrow g(T(X))=g(T(Y))\rightarrow T^*(X)=T^*(Y). 
    \]
\end{solution}

\begin{exercise}
    Establish the following claims. 
    \begin{enumerate}[(a)]
        \item Let \(T\) and \(T^{*}\) be two statistics. Then, there are measurable functions \(f, g\) such that 
        \[
            T=f\left(T^{*}\right)\text{ and }T^{*}=g(T)
        \]
        if and only if \(\sigma(T)=\sigma\left(T^{*}\right)\)
        (i.e., the \(\sigma\)-algebras generated by \(T\) and \(T^{*}\) are identical). 
        
        Hint: recall, from probability theory, that if \(\mathbf{X}\) and \(\mathbf{Y}\) are two random vectors such that \(\mathbf{Y}\) is \(\sigma(\mathbf{X})\)-measurable, then there exists a measurable function \(\varphi\) such that \(\mathbf{Y}=\varphi(\mathbf{X})\). 
        \item Conclude that if we drop the condition " \(\mathcal{P}\)-a.s." in the definition of equivalent statistics, the slightly stronger notion of equivalence we obtain is identical to the equality of the \(\sigma\)-algebras generated by the statistics in question. 
    \end{enumerate}
\end{exercise}

\begin{solution}
    \begin{enumerate}[(a)]
        \item We know that $T$ is $\sigma(T^*)-$measurable and $T^*$ is also $\sigma(T)-$measurable. So, $\sigma(T)=\sigma(T^*)$. 
        
        On the other hand, because $\sigma(T)=\sigma(T^*)$, there are $f$ and $g$, s.t. $T=f(T^*)$, $T^*=g(T)$. 
        \item 
    \end{enumerate}
\end{solution}

\begin{exercise}
    Let \(\mathcal{P}=\{f(\mathbf{x} \mid \theta)\}_{\theta \in \Theta}\) be a parametric family. 
    \begin{enumerate}[(a)]
        \item Prove that \(\mathbf{X}\) is sufficient for \(\theta\). 
        \item In addition, suppose \(\mathcal{P}\) corresponds to an i.i.d. sample. Prove that \(\left(X_{(1)}, \ldots, X_{(n)}\right)^T\) is sufficient for \(\theta\). 
    \end{enumerate}
\end{exercise}

\begin{solution}
    \begin{enumerate}[(a)]
        \item $\prod_{i=1}^nf(x_i|\theta)$, $h(X)=1$, $g(T(X), \theta)=g(X,\theta)=\prod_{i=1}^nf(x_i|\theta)$. So, $X$ is sufficient. 
        \item $\prod_{i=1}^nf(x_{(i)}|\theta)$, $h(X)=1$, $g(T(X), \theta)=g((X_{(1)}, \cdots,X_{(n)})^T,\theta)=\prod_{i=1}^nf(x_{(i)}|\theta)$. So, $(X_{(1)}, \cdots,X_{(n)})^T$ is sufficient. 
    \end{enumerate}
\end{solution}

\begin{exercise}
    Let \(f\) be a positive integrable function over \((0, \infty)\), and let \(p(x \mid \theta)\) be the probability density function over \((0, \theta)\) the density defined by
    \[
        p(x \mid \theta)=\left\{\begin{array}{cl}
        c(\theta) f(x), & 0<x<\theta \\
        0, & \text { otherwise }
        \end{array}\right.
    \]
    If \(X_{1}, \ldots, X_{n}\) are i.i.d. with density \(p(\cdot \mid \theta)\), show that \(X_{(n)}\) is sufficient for \(\theta\). 
\end{exercise}

\begin{solution}
    \[
        \prod_{i=1}^n c(\theta)f(x_i)\mathbf{1}_{x_i<\theta}=c^n(\theta)\prod_{i=1}^nf(x_i)\mathbf{1}_{x_{(n)}<\theta}, 
    \]
    $h(x)=\prod_{i=1}^nf(x_i)$, $g(T(X),\theta)=c^n(\theta)\mathbf{1}_{x_{(n)}<\theta}$. So, $X_{(n)}$ is sufficient. 
\end{solution}

\begin{exercise}
    Let \(f\) be a positive integrable function over \(\mathbb{R}\), and let \(p(x \mid \xi, \eta)\) be the probability density function defined by
    \[
        p(x \mid \xi, \eta)=\left\{\begin{array}{cl}
        c(\xi, \eta) f(x), & \xi<x<\eta \\
        0, & \text { otherwise }
        \end{array}\right.
    \]
    If \(X_{1}, \cdots, X_{n}\) are i.i.d. with density \(p(\cdot \mid \xi, \eta)\), show that \(\left(X_{(1)}, X_{(n)}\right)\) is sufficient for \((\xi, \eta)\).
\end{exercise}


\begin{solution}
    \[
        \prod_{i=1}^np(x_i|\xi, \eta)=\prod_{i=1}^nc(\xi, \eta) f(x_i)\mathbf{1}_{\xi<X_{(1)}, X_{(n)}<\eta}, 
    \]
    $h(x)=\prod_{i=1}^nf(x_i)$, $g(T(X),\xi,\eta)=c^n(\xi,\eta)\mathbf{1}_{\xi<X_{(1)}, X_{(n)}<\eta}$. So, $X_{(1)}, X_{(n)}$ is sufficient for $\xi,\eta$. 
\end{solution}


\begin{exercise}
    \begin{enumerate}[(a)]
        \item Prove that, if \(\mathbf{x}=\left(x_{1}, \ldots, x_{n}\right)\) and \(\mathbf{y}=\left(y_{1}, \ldots, y_{n}\right)\) have the same elementary symmetric functions
        \[
            \sum_{i} x_{i}=\sum_{i} y_{i}, \quad \sum_{i \neq j} x_{i} x_{j}=\sum_{i \neq j} y_{i} y_{j}, \quad \ldots, \quad \prod_{i} x_{i}=\prod_{i} y_{i}
        \]
        then \(\mathbf{y}\) is a permutation of \(\mathbf{x}\). 
        \item Consider a family \(\mathcal{P}=\{f(\cdot \mid \theta)\}_{\theta \in \Theta}\), where \(f(\cdot \mid \theta)\) is an a.c. density for an i.i.d. sample for all \(\theta\), and set
        \[
            T(\mathbf{X})=\left(X_{(1)}, \ldots, X_{(n)}\right)^T .
        \]
        Now consider \(U=\left(U_{1}(X), \ldots, U_{n}(X)\right)^T\), where
        \[
            U_{1}(X)=\sum_{i} X_{i}, \quad U_{2}(X)=\sum_{i \neq j} X_{i} X_{j}, \quad \ldots, \quad U_{n}(X)=\prod_{i} X_{i} .
        \]
        Conclude that \(U\) and \(T\) are equivalent. 
    \end{enumerate}
\end{exercise}

\begin{solution}
    \begin{enumerate}[(a)]
        \item We know that elementary symmetric function $e_i(\cdot)$ is the coefficients for the expansion of the following polynomial: 
        \[
            \prod_{i=1}^n(\lambda-X_i)=\sum_{j=0}^n(-1)^je_j(X_1,\cdots,X_n)\lambda^{n-j}, 
        \]
        $e_0(\cdot)=1$. So, we can get that 
        \[
            \sum_{j=0}^n(-1)^je_j(X_1,\cdots,X_n)\lambda^{n-j}=\sum_{j=0}^n(-1)^je_j(Y_1,\cdots,Y_n)\lambda^{n-j}
        \]
        \[
            \Rightarrow \prod_{i=1}^n(\lambda-X_i)=\prod_{i=1}^n(\lambda-Y_i). 
        \]
        From the multiplicative exchange law, we know that \(\mathbf{y}\) is a permutation of \(\mathbf{x}\). 
        \item 
        \[
            f_1(U(X))=\sum_iX_i=\sum_iX_{(i)}=g_1(T(X)), 
        \]
        \[
            f_2(U(X))=\sum_{i<j}X_iX_j=\sum_{i<j}X_{(i)}X_{(j)}=g_2(T(X)), 
        \]
        \[
            \cdots\cdots
        \]
        \[
            f_n(U(X))=\prod_{i}X_i=\prod_{i}X_{(i)}=g_n(T(X)). 
        \]
        So, $T(X)=(g_1^{-1}\circ f_1,\cdots,g_n^{-1}\circ f_n)(U(X))$, $U(X)=(f_1^{-1}\circ g_1,\cdots,f_n^{-1}\circ g_n)(T(X))$, they are equivalent. 
    \end{enumerate}
\end{solution}

\begin{exercise}
    Show that the order statistics are minimal sufficient for the location family
    \[
        p(\mathbf{x} \mid \theta)=f\left(x_{1}-\theta\right) \ldots f\left(x_{n}-\theta\right)
    \]
    when
    \[
        f(x)=\frac{1}{\pi} \frac{1}{1+x^{2}},
    \]
    i.e., Cauchy \((0,1)\). 
\end{exercise}

\begin{solution}
    \[
        P(X|\theta)=\prod_{i=1}^nf(x_i-\theta)=\pi^{-n}\prod_{i=1}^n(1+(x_i-\theta)^2)^{-1}, 
    \]
    If $x\neq y$, 
    \[
        \frac{P(X|\theta)}{P(Y|\theta)}=\frac{\prod_{i=1}^n(1+(x_i-\theta)^2)^{-1}}{\prod_{i=1}^n(1+(y_i-\theta)^2)^{-1}}, 
    \]
    this ratio must be dependent on $\theta$. So, when $x$ is a permutation of $y$, it is independent on $\theta$. And in this case, $x_{(1)}=y_{(1)}, \cdots, y_{(n)}=y_{(n)}$. 
\end{solution}

\begin{exercise}
    Consider the curved exponential family \(\{n \text { i.i.d. } \mathcal{N}(\theta, \theta)\}_{\theta>0}\). Show that the sufficient statistic
    \[
        T(\mathbf{X})=\left(\sum_{i=1}^{n} X_{i}, \sum_{i=1}^{n} X_{i}^{2}\right)^T
    \]
    is not minimal. 
\end{exercise}

\begin{solution}
    \[
        \begin{aligned} 
            \prod_{i=1}^{n} f\left(x_{i} \mid \theta\right) &=\prod_{i=1}^{n} \frac{1}{\sqrt{2 \pi \theta}} \exp^{-\frac{\left(x_{i}-\theta\right)^{2}}{2 \theta}} \\ &=(2 \pi \theta)^{-\frac{n}{2}} \prod_{i=1}^{n} \exp \left\{-\frac{1}{2 \theta}\left(x_{i}^{2}-2 \theta x_{i}+\theta^{2}\right)\right\} \\ &=(2 \pi \theta)^{-\frac{n}{2}} \exp \left\{-\frac{1}{2 \theta}\left(\sum_{i=1}^{n} x_{i}^{2}-2 \theta \sum_{i=1}^{n} x_{i}+n \theta^{2}\right)\right\} \\ &=(2 \pi \theta)^{-\frac{n}{2}} \exp \left\{\sum_{i=1}^{n} x_{i}\right\} \exp \left\{-\frac{1}{2 \theta} \sum_{i=1}^{n} x_{i}^{2}\right\} \exp \left\{-\frac{n \theta}{2}\right\}, 
        \end{aligned}
    \]
    So, $T^*(X)=\sum_{i=1}^nx_i^2$ is another sufficient statistic. Let $f(x_1,x_2)=x_2$, then 
    \[
        f(T(X))=\sum_{i=1}^nx_i^2 = T^*(X). 
    \]
    But we can not find a function s.t. $g(T^*(X))=T(X)$. Thus, $T(X)$ is not minimal. 
\end{solution}

\begin{exercise}
    Let \(\{f(\mathbf{x} \mid \theta)\}_{\theta \in \mathbb{R}}\) the parametric family corresponding to i.i.d. random variables with joint density function
    \[
        \begin{aligned}
            f(\mathbf{x} \mid \theta)&=C \exp \left\{-\sum_{i=1}^{n}\left(x_{i}-\theta\right)^{4}\right\}\\
            &=C \exp \left(-n \theta^{4}\right) \exp \left\{4 \theta^{3} \sum_{i=1}^{n} x_{i}-6 \theta^{2} \sum_{i=1}^{n} x_{i}^{2}+4 \theta \sum_{i=1}^{n} x_{i}^{3}-\sum_{i=1}^{n} x_{i}^{4}\right\}
        \end{aligned}
    \]
    for some \(C>0\). Express the parameter space and show that
    \[
        T(\mathbf{X})=\left(\sum_{i=1}^{n} X_{i}, \sum_{i=1}^{n} X_{i}^{2}, \sum_{i=1}^{n} X_{i}^{3}\right)^T
    \]
    is a minimal sufficient statistic. 
\end{exercise}

\begin{solution}
    \begin{align}
        \begin{aligned}
        \frac{f(\mathrm{x} \mid \theta)}{f(\mathrm{y} \mid \theta)}=& \exp \left\{4 \theta^{3}\left(\sum_{i=1}^{n} x_{i}-\sum_{i=1}^{n} y_{i}\right)-6 \theta^{2}\left(\sum_{i=1}^{n} x_{i}^{2}-\sum_{i=1}^{n} y_{i}^{2}\right)\right.\\
        &\left.+4 \theta\left(\sum_{i=1}^{n} x_{i}^{3}-\sum_{i=1}^{n} y_{i}^{3}\right)-\left(\sum_{i=1}^{n} x_{i}^{4}-\sum_{i=1}^{n} y_{i}^{4}\right)\right\}, 
        \end{aligned}
    \end{align}
    if we want $\frac{f(\mathrm{x} \mid \theta)}{f(\mathrm{y} \mid \theta)}$ is independent on $\theta$, then we have
    \[
        \left\{\begin{array} { l } 
            { \sum _ { i = 1 } ^ { n } x _ { i } - \sum _ { i = 1 } ^ { n } y _ { i } = 0 } \\
            { \sum _ { i = 1 } ^ { n } x _ { i } ^ { 2 } - \sum _ { i = 1 } ^ { n } y _ { i } ^ { 2 } = 0 } \\
            { \sum _ { i = 1 } ^ { n } x _ { i } ^ { 3 } - \sum _ { i = 1 } ^ { n } y _ { i } ^ { 3 } = 0 }
        \end{array}\right. \quad \Leftrightarrow \quad \left\{\begin{array}{l}
            \sum_{i=1}^{n} x_{i}=\sum_{i=1}^{n} y_{i} \\
            \sum_{i=1}^{n} x_{i}^{2}=\sum_{i=1}^{n} y_{i}^{2} \\
            \sum_{i=1}^{n} x_{i}^{3}=\sum_{i=1}^{n} y_{i}^{3}
        \end{array}\right.
    \]
    So, $T(X)$ must be minimal. 
\end{solution}

\begin{exercise}
    Consider an i.i.d. Poisson family with parameter \(\lambda \in(0, \infty)\), i.e.,
    \[
        f(\mathbf{x} \mid \lambda)=e^{-n \lambda} e^{(\log \lambda) \sum_{i=1}^{n} x_{i}}\left(\prod_{i=1}^{n} x_{i} !\right)^{-1}
    \]
    \begin{enumerate}[(a)]
        \item Conclude that \(\{f(\mathbf{x} \mid \lambda)\}_{\lambda>0}\) is generated by \((T(\mathbf{x}), h(\mathbf{x}))=\left(\sum_{i=1}^{n} x_{i},\left(\prod_{i=1}^{n} x_{i} !\right)^{-1}\right)\), and that \(\mathcal{E}=\mathbb{R}\). Also conclude that \(T(\mathbf{X})\) is minimal sufficient for \(\lambda\). 
        \item Now fix \(\alpha \in \mathbb{R}\) and let \(T_{1}(\mathbf{X})=\alpha T(\mathbf{X}), T_{2}(\mathbf{X})=(1-\alpha) T(\mathbf{X})\). Conclude that \(\{f(\mathbf{x} \mid \lambda)\}_{\lambda>0}\) is generated by \(\left(\left(T_{1}(\mathbf{x}), T_{2}(\mathbf{x})\right) ;\left(\prod_{i=1}^{n} x_{i} !\right)^{-1}\right)\). 
        \item Show that the natural parameter space \(\mathcal{E}^{\prime}\) generated by \(\left(T_{1}(\mathbf{X}), T_{2}(\mathbf{X})\right)\) is \(\mathbb{R}^{2}\). Conclude that \(\left(T_{1}(\mathbf{X}), T_{2}(\mathbf{X})\right)\) is minimal sufficient for \(\lambda\). 
        \item Does (c) contradict the minimal sufficiency of \(T(\mathbf{X})\)? 
    \end{enumerate}
\end{exercise}

\begin{solution}
    \begin{enumerate}[(a)]
        \item \[
                f(x|\lambda)=\frac{1}{(\prod_{i=1}^nx_i!)^{-1}}\exp\left(\log{\lambda} \sum_{i=1}^nx_i-n\lambda\right). 
            \]
            So, it is generated by $\langle \sum_{i=1}^nx, (\prod_{i=1}^nx_i!)^{-1} \rangle$. And 
            \[
                \frac{f(x|\theta)}{f(y|\theta)} = \frac{\prod_{i=1}^ny_i!}{\prod_{i=1}^nx_i!}\exp\left(\log\lambda\left(\sum_{i=1}^n x_i-\sum_{i=1}^n y_i\right)\right). 
            \]
            So, $\sum_{i=1}^n x_i=\sum_{i=1}^n y_i$. $T(x)=\sum_{i=1}^n x_i$ is minimal statistic. 
        \item \[
            f(x|\lambda)=\frac{1}{(\prod_{i=1}^nx_i!)^{-1}}\exp\left(\log{\lambda} (\alpha T_1(X)+(1-\alpha)T_2(X)) - n\lambda\right). 
        \]
        So ,let $\eta=(\alpha\log\lambda, (1-\alpha)\log\lambda)$, then we get the same results. 
        \item $\alpha\in\mathbb{R}$, $\log\lambda\in\mathbb{R}$. So, $\eta\in\mathbb{R}^2$. Similarly, $(T_1(X),T_2(X))$ is also minimal statistic. 
        \item No, $T(X)$ is minimal for $\lambda$; $(T_1(X), T_2(X))$ is minimal for $(\lambda, \alpha)$. 
    \end{enumerate}
    
\end{solution}
