---
title: "Regression Analysis Homework 2"
author: "Zehao Wang"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(deming)
```

Environment Information. 

```{r warning=FALSE}
sessionInfo()
```


**1. Perform a Deming regression using the provided data set (HW2_data1.txt), and compare the results with a regular regression analysis. **

Note: yy is the observed outcome, and xx is the observed predictor, and x_star is the “unobserved” true x value. Your analysis should be based on the observed data (xx and yy), and x_star could be used to evaluate the effectiveness of Deming regression in taking errors-in-variable into consideration. 

```{r message=FALSE}
data_1 <- read_table("HW2_data1.txt")
```

```{r}
lm_deming <- deming(yy ~ xx, data_1)
print(lm_deming)
```

```{r}
lm_1 <- lm(yy ~ xx, data_1)
summary(lm_1)
```

Deming Regression: 
\[Y=0.437X+2.614\]
Regular Regression: 
\[Y=0.343X+3.022\]

```{r}
ggplot() +
  geom_point(data = data_1, mapping = aes(xx, yy, color = "x with error")) +
  geom_point(data = data_1, mapping = aes(x_star, yy, color = "x without error")) +
  geom_line(data = tibble(
    x = c(-4.5:14.5),
    y = c(-4.5:14.5) * 0.437 + 2.614
  ),
  mapping = aes(x, y, color = "Deming")) +
  geom_line(data = tibble(
    x = c(-4.5:14.5),
    y = c(-4.5:14.5) * 0.343 + 3.022
  ),
  mapping = aes(x, y, color = "Regular")) + xlab("x") + ylab("y") + theme_bw()
```

Now, calculate the true SSE: 
```{r}
sum((data_1$x_star * 0.437 + 2.614 - data_1$yy) ^ 2)
```

\[SSE_{Deming}=95.11144. \]


```{r}
sum((data_1$x_star * 0.343 + 3.022 - data_1$yy) ^ 2)
```

\[SSE_{Regular}=124.3684.  \]

We can see that Deming Regression has samller SSE. 

**2. For the following matrices and vectors, manually conduct the indicated operations (you can use software to verify your results if you want). **

\[A=\left(\begin{matrix}
1&1\\
2&0\\
3&1
\end{matrix}\right), 
B=\left(\begin{matrix}
1&1\\
2&0
\end{matrix}\right), 
C=\left(\begin{matrix}
1&1&1\\
2&0&1\\
3&1&0
\end{matrix}\right), 
D=\left(\begin{matrix}
1&1&1\\
2&0&1\\
3&1&2
\end{matrix}\right). \]


**A. Find the determinants of C and D. Note the relationships across columns in C or D. Particularly, any individual column in C cannot be obtained as a linear combination of other columns in C. However, the third column of D can be obtained as the average of first two columns (or in other words, ½*first column + ½*second column). This illustrates the situation when a matrix has a zero determinant.**

For $C$, 
\[det(C)=1\cdot\left|\begin{matrix}2&0\\3&1\end{matrix}\right|-1\cdot\left|\begin{matrix}1&1\\3&1\end{matrix}\right|=2-(-2)=4.\]

For $D$, 
\[det(D)=-1\cdot\left|\begin{matrix}2&1\\3&2\end{matrix}\right|-1\cdot\left|\begin{matrix}1&1\\2&1\end{matrix}\right|=-1-(-1)=0.\]

**B. 	Compute AB.**

\[
AB=\left(\begin{matrix}
1+2&1+0\\
2+0&2+0\\
3+2&3+0
\end{matrix}\right)=\left(\begin{matrix}
3&1\\
2&2\\
5&3
\end{matrix}\right)
\]

**C.	Find the inverse of B.**

\[B=\left(\begin{matrix}
1&1\\
2&0
\end{matrix}\right), I=\left(\begin{matrix}
1&0\\
0&1
\end{matrix}\right)\]
First column times $-1$ then plus the second column: 
\[B=\left(\begin{matrix}
1&0\\
2&-2
\end{matrix}\right), I=\left(\begin{matrix}
1&-1\\
0&1
\end{matrix}\right)\]
Second column plus the first column: 
\[B=\left(\begin{matrix}
1&0\\
0&-2
\end{matrix}\right), I=\left(\begin{matrix}
0&-1\\
1&1
\end{matrix}\right)\]
Second column times $-1/2$:
\[B=\left(\begin{matrix}
1&0\\
0&1
\end{matrix}\right), I=\left(\begin{matrix}
0&1/2\\
1&-1/2
\end{matrix}\right)\]
So, $B^{-1}=\left(\begin{matrix}0&1/2\\1&-1/2\end{matrix}\right)$. 

**3. In a diabetes study, 1123 subjects were recruited, and a number of clinical traits and information were collected, including (see data in the attached file “HW2_data2.txt”, which is the same data set used for HW1):**

Consider a multiple linear regression model between hba1c and fbg + tg. 

```{r message=FALSE}
data_2 <- read_table("HW2_data2.txt")
```

**a. Given that $X'Y=\left(\begin{matrix}7622\\55937\\13830\end{matrix}\right)$  and $(X'X)^{-1}=\left(\begin{matrix}0.0091&-0.00085&-0.0014\\-0.00085&0.00015&-0.00011\\-0.0014&-0.00011&0.0012\end{matrix}\right)$(with the order of intercept, fbg, tg, in the model). Manually obtain estimates for the intercept, and coefficients for fbg and tg. **

\[\hat{\beta}=(X'X)^{-1}(X'Y)=\left[\begin{matrix}
0.0091\times7622-0.00085\times55937-0.0014\times13830\\
-0.00085\times7622+0.00015\times55937-0.00011\times13830\\
-0.0014\times7622-0.00011\times55937+0.0012\times13830
\end{matrix}\right]=\left[\begin{matrix}
2.45175\\
0.39055\\
-0.22787
\end{matrix}\right]\]
So, the estimates for intercept is $2.45175$; fbg is $0.39055$; tg is $-0.22787$. (There is some error with the true value of the estimated coefficient which is becausse the loss of precision for these two matrix. )

**b. Manually find the variances for b_1 and b_2, and their correlation coefficient. Check your results using software of your choice. **

\[Var(\hat{\beta})=Var((X'X)^{-1}X'Y)=((X'X)^{-1}X')Var(Y)(X(X'X)^{-1})=Var(Y)(X'X)^{-1}=\sigma^2(X'X)^{-1}.\]

```{r}
lm_2 <- lm(hba1c ~ fbg + tg, data_2)
sum(lm_2$residuals ^ 2) / (length(data_2) - 3)
```


\[\hat{\sigma}^2=\frac{SSE}{n-2}=266.1585\]

\[Var(\hat{\beta})=\sigma^2(X'X)^{-1}=\left(\begin{matrix}
2.4220424& -0.22623472& -0.37262190\\
-0.2262347&  0.03992377& -0.02927744\\
-0.3726219& -0.02927744&  0.31939020\end{matrix}\right)\]

So, $Var(b_1)=0.0399$, $Var(b_2)=0.3194$. 
\[
Cor(b_1,b_2)=\frac{Cov(b_1,b_2)}{\sigma_1\sigma_2}=\frac{-0.02927744}{\sqrt{0.03992377}\sqrt{0.31939020}}=-0.2592725.  
\]

```{r}
summary(lm_2)
```

Compared with the results generated by software, the error is pretty large. 

**c. For the two ways in entering the predictors into the model (fbg first, then tg, vs tg first, then fbg), summarize in a table for the sequential and partial sums of squares, respectively, for fbg and tg. **

```{r}
anova(lm(hba1c ~ tg + fbg, data_2))
```

```{r}
anova(lm(hba1c ~ fbg + tg, data_2))
```

| Source of variation 	| df 	| Sum of Squares 	| MS 	|
|:---:	|:---:	|:---:	|:---:	|
| fbg first 	| 1 	| 2796.47 	| 2796.47 	|
| tg second 	| 1 	| 0.23 	| 0.23 	|
| tg first 	  | 1 	| 181.529 	| 181.529 	|
| fbg second 	| 1 	| 2615.18 	| 2615.18 	|
| SSE 	| 1120 	| 1863.11 	| 1.66 	|
| SST 	| 1122 	| 4659.82 	|  	|


