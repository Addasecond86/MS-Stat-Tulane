\documentclass[en, normal, 11pt, black]{elegantnote}

\usepackage{tcolorbox}
\tcbuselibrary{breakable}
\usepackage{amsfonts} 
\usepackage{newtxtext}
\usepackage{ulem}
\usepackage{amssymb}

\newenvironment{exercise}[1]{\begin{tcolorbox}[colback=black!15, colframe=black!80, breakable, title=#1]}{\end{tcolorbox}}

\renewenvironment{proof}{\begin{tcolorbox}[colback=white, colframe=black!50, breakable, title=Proof. ]\setlength{\parskip}{0.8em}}{\,\\\rightline{$\square$}\end{tcolorbox}}

\newenvironment{solution}{\begin{tcolorbox}[colback=white, colframe=black!50, breakable, title=Solution. ]\setlength{\parskip}{0.8em}}{\end{tcolorbox}}

\newcommand{\pder}{\partial\,}

\newcommand{\der}{\,\mathbf{d}}

\title{\textsc{Probability: Problem Set 4}}
\author{\textsc{Zehao Wang}}
\date{\today}


% \vspace{-30pt}

\begin{document}
    \maketitle
    \begin{exercise}{2.1.1}
        Suppose $\left(X_{1}, \ldots, X_{n}\right)$ has density $f\left(x_{1}, x_{2}, \ldots, x_{n}\right)$, that is
        \[
        P\left(\left(X_{1}, X_{2}, \ldots, X_{n}\right) \in A\right)=\int_{A} f(x) d x \text { for } A \in \mathcal{R}^{n}
        \]
        If $f(x)$ can be written as $g_{1}\left(x_{1}\right) \cdots g_{n}\left(x_{n}\right)$ where the $g_{m} \geq 0$ are measurable, then $X_{1}, X_{2}, \ldots, X_{n}$ are independent. Note that the $g_{m}$ are not assumed to be probability densities. 
    \end{exercise}

    \begin{exercise}{2.1.2}
         Suppose $X_{1}, \ldots, X_{n}$ are random variables that take values in countable sets $S_{1}, \ldots, S_{n} .$ Then in order for $X_{1}, \ldots, X_{n}$ to be independent, it is sufficient that whenever $x_{i} \in S_{i}$
         \[
             P\left(X_{1}=x_{1}, \ldots, X_{n}=x_{n}\right)=\prod_{i=1}^{n} P\left(X_{i}=x_{i}\right). 
         \]
    \end{exercise}

    \begin{exercise}{2.1.3}
        Let $\rho(x, y)$ be a metric. 
        
        (i) Suppose $h$ is differentiable with $h(0)=0, h^{\prime}(x)>0$ for $x>0$, and $h^{\prime}(x)$ decreasing on $[0, \infty)$. Then $h(\rho(x, y))$ is a metric. 
        
        (ii) $h(x)=x /(x+1)$ satisfies the hypotheses in (i). 
    \end{exercise}

    \begin{exercise}{2.1.4}
        Let $\Omega=(0,1), \mathcal{F}=$ Borel sets, $P=$ Lebesgue measure. $X_{n}(\omega)=\sin (2 \pi n \omega)$, $n=1,2, \ldots$ are uncorrelated but not independent. 
    \end{exercise}

    \begin{exercise}{2.1.6}
        Prove directly from the definition that if $X$ and $Y$ are independent and $f$ and $g$ are measurable functions, then $f(X)$ and $g(Y)$ are independent.
    \end{exercise}

    \begin{exercise}{2.1.7}
        Let $K \geq 3$ be a prime and let $X$ and $Y$ be independent random variables that are uniformly distributed on $\{0,1, \ldots, K-1\} .$ For $0 \leq n<K$, let $Z_{n}=X+$ $n Y \bmod K$. Show that $Z_{0}, Z_{1}, \ldots, Z_{K-1}$ are pairwise independent, i.e., each pair is independent. They are not independent because if we know the values of two of the variables then we know the values of all the variables. 
    \end{exercise}
    
    \begin{exercise}{2.1.9}
        Let $\Omega=\{1,2,3,4\}, \mathcal{F}=$ all subsets of $\Omega$, and $P(\{i\})=1 / 4$. Give an example of two collections of sets $\mathcal{A}_{1}$ and $\mathcal{A}_{2}$ that are independent but whose generated $\sigma$-fields are not. 
    \end{exercise}

    \begin{exercise}{2.1.10}
        Show that if $X$ and $Y$ are independent, integer-valued random variables, then
        \[
            P(X+Y=n)=\sum_{m} P(X=m) P(Y=n-m). 
        \]
    \end{exercise}

    \begin{exercise}{2.1.11}
        In Example $1.6 .13$, we introduced the Poisson distribution with parameter $\lambda$, which is given by $P(Z=k)=e^{-\lambda} \lambda^{k} / k !$ for $k=0,1,2, \ldots$ Use the previous exercise to show that if $X=$ Poisson $(\lambda)$ and $Y=$ Poisson $(\mu)$ are independent, then $X+Y=$ $\operatorname{Poisson}(\lambda+\mu)$. 
    \end{exercise}

    \begin{exercise}{2.1.12}
        $X$ is said to have a Binomial $(n, p)$ distribution if
        \[
            P(X=m)=\binom{n}{m}p^{m}(1-p)^{n-m}. 
        \]
    (i) Show that if $X=\operatorname{Binomial}(n, p)$ and $Y=\operatorname{Binomial}(m, p)$ are independent then $X+Y=\operatorname{Binomial}(n+m, p)$. 
    
    (ii) Look at Example $1.6 .12$ and use induction to conclude that the sum of $n$ independent Bernoulli $(p)$ random variables is $\operatorname{Binomial}(n, p)$. 
    \end{exercise}

    \begin{exercise}{2.1.14}
        Let $X, Y \geq 0$ be independent with distribution functions $F$ and $G$. Find the distribution function of $X Y$. 
    \end{exercise}

\end{document}
