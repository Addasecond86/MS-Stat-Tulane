\documentclass[en, normal, 11pt, black]{elegantnote}

\usepackage{tcolorbox}
\tcbuselibrary{breakable}
\usepackage{amsfonts} 
\usepackage{newtxtext}
\usepackage{ulem}
\usepackage{amssymb}

\newenvironment{exercise}[1]{\begin{tcolorbox}[colback=black!15, colframe=black!80, breakable, title=#1]}{\end{tcolorbox}}

\renewenvironment{proof}{\begin{tcolorbox}[colback=white, colframe=black!50, breakable, title=Proof. ]\setlength{\parskip}{0.8em}}{\,\\\rightline{$\square$}\end{tcolorbox}}

\newenvironment{solution}{\begin{tcolorbox}[colback=white, colframe=black!50, breakable, title=Solution. ]\setlength{\parskip}{0.8em}}{\end{tcolorbox}}

\newcommand{\pder}{\partial\,}

\newcommand{\der}{\,\mathbf{d}}

\title{\textsc{Probability: Problem Set 3}}
\author{\textsc{Zehao Wang}}
\date{Oct 10, 2021}

% \vspace{-30pt}

\begin{document}
    \maketitle
    \begin{exercise}{1.6.1}
        Suppose $\varphi$ is strictly convex, i.e., $>$ holds for $\lambda \in(0,1)$. Show that, under the assumptions of Theorem 1.6.2, $\varphi(E X)=E \varphi(X)$ implies $X=E X$ a.s. 
    \end{exercise} 

    \begin{proof}
        Because $\varphi$ is strictly convex, there exist $c\in \mathbb{R}$, such that when $x\not=x_0$, 
        \[
            \varphi(x)-\varphi(x_0)>c(x-x_0), 
        \]
        Let $x_0=EX$, we can get that 
        \[
            \varphi(x)>c(x-EX)+\varphi(EX), 
        \]
        From Jensen's inequality, we know that if $E\varphi(X)=\varphi(EX)$, then $E(x-EX)=0$, i.e. $P(X=EX)=1$, $X=EX$, a.e. 
    \end{proof}

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \begin{exercise}{1.6.2}
        Suppose $\varphi: \mathbb{R}^{n} \rightarrow \mathbb{R}$ is convex. Imitate the proof of Theorem $1.5.1$ to show
        \[
            E \varphi\left(X_{1}, \ldots, X_{n}\right) \geq \varphi\left(E X_{1}, \ldots, E X_{n}\right). 
        \]
        provided $E\left|\varphi\left(X_{1}, \ldots, X_{n}\right)\right|<\infty$ and $E\left|X_{i}\right|<\infty$ for all $i$. 
    \end{exercise}

    \begin{proof}
        Because $\varphi$ is convex, let $X$ denote $(X_1, \cdots, X_n)$ and we can get a linear function $l(X)=\varphi(E(X))+\sum_{i=1}^na_i(X_i-E(X_i))$, which satisfy for some $c=E(X)$, $l(c)=\varphi(c)$, and for other $x\not=c$, $\varphi(X)>l(X)$, 
        \[
            E(\varphi(X))\geqslant E\left(l(X)\right)=\varphi(E(X))+\sum_{i=1}^na_i(E(X_i)-E(X_i))=\varphi(E(X)). 
        \]
        \vspace{-30pt}
    \end{proof}

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \begin{exercise}{1.6.3. Chebyshev's inequality is and is not sharp.}        
        (i) Show that Theorem $1.6.4$ is sharp by showing that if $0<b \leq a$ are fixed there is an $X$ with $E X^{2}=b^{2}$ for which $P(|X| \geq a)=b^{2} / a^{2}$. 
        
        (ii) Show that Theorem 1.6.4 is not sharp by showing that if $X$ has $0<E X^{2}<\infty$, then
        \[
            \lim _{a \rightarrow \infty} a^{2} P(|X| \geq a) / E X^{2}=0. 
        \]
    \end{exercise}

    \begin{proof}
        (i), Let 
        \[
            P(|X|=a)=2P(X=a)=2P(X=-a)=\frac{b^2}{a^2}, 
        \]
        and $P(X=0)=1-P(|X|=a)$. So, $E(X)=0$, $E(X^2)=Var(X)=b^2$. If $i_{A}=\inf\,\{\varphi(y): y \in A\}$, then we need to prove that
        \[
            i_{A}\,P(X \in A) \leq E(\varphi(X) ; X \in A) \leq E \varphi(X), 
        \]
        where $A=\{X:P(|X|\geqslant a)\}$, $\varphi(X)=X^2$. So, we have
        \begin{align*}
            &i_{A}\,P(X \in A)=a^2\frac{b^2}{a^2}=b^2, \\
            &E(\varphi(X) ; X \in A)=a^2\frac{b^2}{a^2}=b^2, \\
            &E \varphi(X)=b^2. 
        \end{align*}
        So, it is sharp for such a r.v. $X$. 
        
        (ii), WLOG, let $X\sim N(0,1)$, then $EX=0$, $Var(X)=E(X^2)=1$. $A_a=\{X:P(|X|\geqslant a)\}$, then
        \[
            a^{2} P(|X| \geq a) / E X^{2}\leqslant x^2 P(|X| \geq a)=\int_{a}^\infty x^2 f(x)\der x, 
        \]
        where $f(\cdot)$ is pdf of $X$. So, 
        \[
            \lim_{a\to\infty}a^{2} P(|X| \geq a) / E X^{2}\longrightarrow 0, 
        \]
        And because $a^{2} P(|X| \geq a)\leqslant E(X^2)$, for any $a$, Chebyshevâ€™s inequality is not sharp. 
    \end{proof}

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \begin{exercise}{1.6.9. Inclusion-exclusion formula.}
        Let $A_{1}, A_{2}, \ldots A_{n}$ be events and $A=\cup_{i=1}^{n} A_{i}$. Prove that $\mathbf{1}_{A}=1-\prod_{i=1}^{n}\left(1-\mathbf{1}_{A_{i}}\right)$. Expand out the right-hand side, then take expected value to conclude
        \begin{align*}
            P\left(\cup_{i=1}^{n} A_{i}\right)=& \sum_{i=1}^{n} P\left(A_{i}\right)-\sum_{i<j} P\left(A_{i} \cap A_{j}\right) \\
            &+\sum_{i<j<k} P\left(A_{i} \cap A_{j} \cap A_{k}\right)-\cdots+(-1)^{n-1} P\left(\cap_{i=1}^{n} A_{i}\right). 
        \end{align*}
    \end{exercise}

    \begin{proof}
        If $x\in A$, i.e. $x\in \cup_{i=1}^\infty A_i$. There must be at least one $i$, such that $x\in A_i$, $1-\mathbf{1}_{A_i}=0$. So, in this case, 
        \[
            \mathbf{1}_A=1=1-0=1-\prod_{i=1}^n(1-\mathbf{1}_{A_i}). 
        \] 
        If $x\not\in A$, the $\mathbf{1}_A=0$, and $\mathbf{1}_{A_i}=0$, for all $i$. Then
        \[
            \mathbf{1}_{A}=0=1-1=1-\prod_{i=1}^n(1-0)=1-\prod_{i=1}^n(1-\mathbf{1}_{A_i}). 
        \]
        For $\prod_{i=1}^n(1-\mathbf{1}_{A_i})$, we can expand it as
        \begin{align*}
            \mathbf{1}_A=&1-\prod_{i=1}^n(1-\mathbf{1}_{A_i})\\
            =&1-(1-\mathbf{1}_{A_1})(1-\mathbf{1}_{A_2})\cdots(1-\mathbf{1}_{A_n})\\
            =&(-1)^{0}\sum_{1\leqslant i\leqslant n}\mathbf{1}_{A_i}+(-1)^{1}\sum_{1\leqslant i<j\leqslant n}\mathbf{1}_{A_i}\mathbf{1}_{A_j}\\
            &+(-1)^{2}\sum_{1\leqslant i<j<k\leqslant n}\mathbf{1}_{A_i}\mathbf{1}_{A_j}\mathbf{1}_{A_k}-\cdots+(-1)^{n-1}\mathbf{1}_{A_1}\cdots\mathbf{1}_{A_n}\\
            =&\sum_{1\leqslant i\leqslant n}\mathbf{1}_{A_i}-\sum_{1\leqslant i<j\leqslant n}\mathbf{1}_{A_i}\mathbf{1}_{A_j}
            +\sum_{1\leqslant i<j<k\leqslant n}\mathbf{1}_{A_i}\mathbf{1}_{A_j}\mathbf{1}_{A_k}-\cdots+(-1)^{n-1}\mathbf{1}_{A_1}\cdots\mathbf{1}_{A_n}, 
        \end{align*}
        Take expectation for both sides, we can get
        \begin{align*}
            P(A)=P(\cup_{i=1}^nA_i)=&\sum_{1\leqslant i\leqslant n}P(A_i)-\sum_{1\leqslant i<j\leqslant n}P(A_i\cap A_j)\\
            &+\sum_{1\leqslant i<j<k\leqslant n}P(A_i\cap A_j\cap A_k)-\cdots+(-1)^{n-1}P(A_1\cap\cdots\cap A_n). 
        \end{align*}
        \vspace{-30pt}
    \end{proof}

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \begin{exercise}{1.6.10. Bonferroni inequalities.}
        Let $A_{1}, A_{2}, \ldots A_{n}$ be events and $A=\cup_{i=1}^{n} A_{i} .$ Show that $\mathbf{1}_{A} \leq \sum_{i=1}^{n} \mathbf{1}_{A_{i}}$, etc. and then take expected values to conclude
        \[
            P\left(\cup_{i=1}^{n} A_{i}\right) \leq \sum_{i=1}^{n} P\left(A_{i}\right), 
        \]
        \[
            P\left(\cup_{i=1}^{n} A_{i}\right) \geq \sum_{i=1}^{n} P\left(A_{i}\right)-\sum_{i<j} P\left(A_{i} \cap A_{j}\right), 
        \]
        \[
            P\left(\cup_{i=1}^{n} A_{i}\right) \leq \sum_{i=1}^{n} P\left(A_{i}\right)-\sum_{i<j} P\left(A_{i} \cap A_{j}\right)+\sum_{i<j<k} P\left(A_{i} \cap A_{j} \cap A_{k}\right). 
        \]
        In general, if we stop the inclusion-exclusion formula after an even (odd) number of sums, we get a(n) lower (upper) bound. 
    \end{exercise}
    \begin{proof}
        Becasue subadditivity, first inequality is apparent. For the second inequality, it equals to 
        \[
            \mathbf{1}_{A} \geqslant \sum_{1\leqslant i\leqslant n}^{n} \mathbf{1}_{A_{i}}-\sum_{1\leqslant i<j\leqslant n} \mathbf{1}_{A_{i}} \mathbf{1}_{A_{j}}, 
        \]
        Assume for $x\in A$, there are $m$ sets $A_i, \cdots, A_{i+m-1}$ satisfy $x\in A_i$, $m\geqslant2$. Then
        \[
            \sum_{1\leqslant i\leqslant n}^{n} \mathbf{1}_{A_{i}}-\sum_{1\leqslant i<j\leqslant n} \mathbf{1}_{A_{i}} \mathbf{1}_{A_{j}}
            =m-\binom{m}{2} \leqslant 1=\mathbf{1}_{A}. 
        \]
        Similarly, for the third inequality, we need to prove
        \[
            \mathbf{1}_{A} \leqslant \sum_{1\leqslant i\leqslant n}^{n} \mathbf{1}_{A_{i}}-\sum_{1\leqslant i<j\leqslant n} \mathbf{1}_{A_{i}} \mathbf{1}_{A_{j}}+\sum_{1\leqslant i<j<k\leqslant n} \mathbf{1}_{A_{i}} \mathbf{1}_{A_{j}}\mathbf{1}_{A_{k}}, 
        \]
        And if $m\geqslant3$, 
        \[
            \sum_{1\leqslant i\leqslant n}^{n} \mathbf{1}_{A_{i}}-\sum_{1\leqslant i<j\leqslant n} \mathbf{1}_{A_{i}} \mathbf{1}_{A_{j}}+\sum_{1\leqslant i<j<k\leqslant n} \mathbf{1}_{A_{i}} \mathbf{1}_{A_{j}}\mathbf{1}_{A_{k}}=m-\binom{m}{2}+\binom{m}{3}\geqslant1. 
        \]
        \vspace{-30pt}
    \end{proof}

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \begin{exercise}{1.6.11.}
        If $E|X|^{k}<\infty$, then for $0<j<k, E|X|^{j}<\infty$, and furthermore 
        \[
            E|X|^{j} \leq\left(E|X|^{k}\right)^{j / k}. 
        \]
    \end{exercise}

    \begin{proof}
        When $|X|\leqslant1$, $|X|^j\geqslant|X|^k$. So, $E|X|^j\leqslant\infty$. If $|X|^j\leqslant|X|^k$, then because $E|X|^k\leqslant\infty$, $E|X|^j\leqslant\infty$. For the inequality, let $\varphi(x)=x^{k/j}$. And because $\varphi$ is convex, we have
        \[
            E|X|^{k}=E\varphi\left(|X|^j\right)\geqslant\varphi\left(E|X|^j\right), 
        \]
        \[
            E|X|^{k}\geqslant\left(E|X|^j\right)^{k/j}, 
        \]
        \[
            E|X|^j\leqslant \left(E|X|^{k}\right)^{j/k}. 
        \]
        \vspace{-30pt}
    \end{proof}

\end{document}
