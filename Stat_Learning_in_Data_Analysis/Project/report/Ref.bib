@article{Cortes1995,
  abstract = {The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data. High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition. © 1995, Kluwer Academic Publishers. All rights reserved.},
  author   = {Corinna Cortes and Vladimir Vapnik},
  doi      = {10.1023/A:1022627411411},
  issn     = {15730565},
  issue    = {3},
  journal  = {Machine Learning},
  title    = {Support-Vector Networks},
  volume   = {20},
  year     = {1995}
}
@article{Berkson1944,
  author  = {Joseph Berkson},
  doi     = {10.1080/01621459.1944.10500699},
  issn    = {1537274X},
  issue   = {227},
  journal = {Journal of the American Statistical Association},
  title   = {Application of the Logistic Function to Bio-Assay},
  volume  = {39},
  year    = {1944}
}

@article{Akaike1974,
  abstract = {The history of the development of statistical hypothesis testing in time series analysis is reviewed briefly and it is pointed out that the hypothesis testing procedure is not adequately defined as the procedure for statistical model identification. The classical maximurn likelihood estimation procedure is reviewed and a new estimate minimum information theoretical criterion (AlC) estimate (MAICE) which is designed for the purpose of statistical identification is introduced. When there are several competing models the MAICE is defined by the model and the maximum likelihood estimates of the parameters which give the minimum of AIC defined by AIC = (-2)log-(maximum likelihood) + 2(number of independently adjusted parameters within the model). MAICE provides a versatile procedure for statistical model identification which is free from the ambiguities inherent in the application of conventional hypothesis testing procedure. The practical utility of MAICE in time series analysis is demonstrated with some numerical examples. © 1974, IEEE. All rights reserved.},
  author   = {Hirotugu Akaike},
  doi      = {10.1109/TAC.1974.1100705},
  issn     = {15582523},
  issue    = {6},
  journal  = {IEEE Transactions on Automatic Control},
  title    = {A New Look at the Statistical Model Identification},
  volume   = {19},
  year     = {1974}
}

