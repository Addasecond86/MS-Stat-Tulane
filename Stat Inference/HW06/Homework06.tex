\documentclass[14pt]{elegantbook}

\newcommand{\CN}{BIOS 7040\\[0.5cm] Statistical Inference I}
\newcommand{\Ti}{Homework 6}
\newcommand{\Pf}{Dr. Srivastav}
\newcommand{\FN}{Zehao}
\newcommand{\LN}{Wang}
\usepackage[fontsize=14pt]{fontsize}

\usepackage{enumitem}
\renewcommand{\chaptername}{Homework}

\allowdisplaybreaks
\begin{document}
\include{title.tex}
\setcounter{chapter}{5}
\chapter{}
    \setcounter{chapter}{3}
    \setcounter{exer}{15}
    \begin{exercise}
        Verify these two identities regarding the gamma function that we given in the text: 
        \begin{enumerate}[(a)]
            \item $\Gamma(\alpha+1) = \alpha\Gamma(\alpha)$. 
            \item $\Gamma(1/2)=\sqrt{\pi}$. 
        \end{enumerate}
    \end{exercise}

    \begin{solution}
        \begin{enumerate}[(a)]
            \item From the definition of the gamma function, we have
            \begin{align*}
                \Gamma(\alpha+1)&=\int_{0}^{\infty}t^\alpha e^{-t}dt\\
                &=\int_{0}^{\infty}-t^\alpha d(e^{-t})\\
                &=\left.-t^\alpha e^{-t}\right|_0^\infty-\int_{0}^{\infty}-\alpha t^{\alpha-1} e^{-t}dt\\
                &=\lim_{t\to\infty}\frac{-t^\alpha}{e^t}+\alpha\int_{0}^{\infty} t^{\alpha-1} e^{-t}dt\\
                &=0+\alpha\Gamma(\alpha)=\alpha\Gamma(\alpha).
            \end{align*}
            \item With definition, we have
            \begin{align*}
                \Gamma(1/2)&=\int_{0}^{\infty}t^{1/2-1} e^{-t}dt\\
                &=\int_{0}^{\infty}t^{-1/2} e^{-t}dt
            \end{align*}
            Substituting $x=t^{1/2}$, $t=x^2$, $dt=2xdx$. We have
            \begin{align*}
                \Gamma(1/2)&=\int_{0}^{\infty}x^{-1} e^{-x^2}2xdx\\
                &=2\int_{0}^{\infty} e^{-x^2}dx
            \end{align*}
            Because $e^{-x^2}$ is an even function, we have
            \begin{align*}
                \Gamma(1/2)&=\int_{-\infty}^{\infty} e^{-x^2}dx
            \end{align*}
            Now, we square both sides and get that 
            \begin{align*}
                \Gamma^2(1/2)&=\int_{-\infty}^{\infty} e^{-x^2}dx\int_{-\infty}^{\infty} e^{-x^2}dx\\
                &=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} e^{-(x^2+y^2)}dxdy
            \end{align*}
            Switch to Polar coordinate system: \[x=r\cos\theta, \qquad y=r\sin\theta\]
            \[
                |J|=\left|\begin{matrix}
                \cos\theta & -r\sin\theta\\
                \sin\theta & r\cos\theta
            \end{matrix}\right|=r
            \]
            we have
            \begin{align*}
                \Gamma^2(1/2)&=\int_{0}^{2\pi}\int_{0}^{\infty} e^{-r^2}rdrd\theta
            \end{align*}
            Substituting $r^2=u$, $du=2rdr$, we have
            \begin{align*}
                \Gamma^2(1/2)&=\int_{0}^{2\pi}\int_{0}^{\infty} e^{-u}\frac{1}{2}dud\theta\\
                &=\int_{0}^{2\pi}\frac{1}{2}\left.-e^{-u}\right|_0^\infty d\theta\\
                &=\frac{1}{2}\int_{0}^{2\pi}\mathbf{1}d\theta\\
                &=\frac{1}{2}\left.\theta\right|_0^{2\pi}\\
                &=\pi. 
            \end{align*}
            So, we have $\Gamma(1/2)=\sqrt{\pi}$.
        \end{enumerate}
    \end{solution}

    \setcounter{exer}{21}
    \begin{exercise}
        For each of the following distributions, verify the formula for $EX$ and $VarX$ given in the text. 
        \begin{enumerate}[(a)]
            \item Verify $VarX$ if $X$ has a $Poisson(\lambda)$ distribution. (Hint: Compute $EX(X-1)=EX^2-EX$). 
            \item Verify $VarX$ if $X$ has a negative binomial distribution$(r,p)$. 
            \item Verify $VarX$ if $X$ has a gamma distribution$(\alpha, \beta)$. 
        \end{enumerate}
    \end{exercise}

    \begin{solution}
        \begin{enumerate}[(a)]
            \item First, Calculate $EX(X-1)$: 
            \begin{align*}
                EX(X-1)&=\sum_{k=0}^{\infty}k(k-1)P(X=k)\\
                &=\sum_{k=2}^{\infty}k(k-1)P(X=k)\\
                &=\sum_{k=2}^{\infty}k(k-1)\frac{\lambda^k}{k!}e^{-\lambda}\\
                &=\sum_{k=2}^{\infty}\frac{\lambda^k}{(k-2)!}e^{-\lambda}\\
                &=\lambda^2\sum_{k=2}^{\infty}\frac{\lambda^{k-2}}{(k-2)!}e^{-\lambda}\\
                &=\lambda^2. 
            \end{align*}
            And \begin{align*}
                EX&=\sum_{k=0}^{\infty}kP(X=k)\\
                &=\sum_{k=0}^{\infty}k\frac{\lambda^k}{k!}e^{-\lambda}\\
                &=\lambda\sum_{k=1}^{\infty}\frac{\lambda^{k-1}}{(k-1)!}e^{-\lambda}\\
                &=\lambda.
            \end{align*}
            So, $EX^2=\lambda^2+EX=\lambda^2+\lambda$. Then, 
            \[Var(X)=EX^2-(EX)^2=\lambda^2+\lambda-\lambda^2=\lambda. \]
            \item \label{3.22.b} I think we can use the Theory of Exponential Family in 3.4. For negative binomial distribution, we have
            \begin{align*}
                P(X=x)&=\frac{(x+r-1)!}{x!(r-1)!}p^r(1-p)^x\\
                &=\frac{(x+r-1)!}{x!(r-1)!}p^r\exp\left(x\ln(1-p)\right). 
            \end{align*}
            Then, $h(x)=\frac{(x+r-1)!}{x!(r-1)!}$, $c(p, r)=p^r$, and $w(p)=\ln(1-p)$, $t(x)=x$. Using the Theorem 3.4.2, we have
            \[
                E\left(\sum_{i=1}^k\frac{\partial\,w_i(\theta)}{\partial\,\theta_j}t_i(X)\right)=-\frac{\partial}{\partial\,\theta_j}\log(c(\theta)).
            \]
            So, the Expectation of $X$ is: 
            \begin{align*}
                E\left(\frac{\partial\,\ln(1-p)}{\partial\,p}X\right)&=-\frac{\partial\,r\ln(p)}{\partial\,p}\\
                E\left(\frac{1}{p-1}X\right)&=-\frac{r}{p}\\
                E(X)&=\frac{r(1-p)}{p}. 
            \end{align*}
            And also, 
            \[
                Var\left(\sum_{i=1}^k\frac{\partial\,w_i(\theta)}{\partial\,\theta_j}t_i(X)\right)=-\frac{\partial^2}{\partial\,\theta_j^2}\log(c(\theta))-E\left(\sum_{i=1}^k\frac{\partial^2\,w_i(\theta)}{\partial\,\theta_j^2}t_i(X)\right).
            \]
            So, 
            \begin{align*}
                Var\left(\frac{\partial\,\ln(1-p)}{\partial\,p}X\right)&=-\frac{\partial^2\,r\ln(p)}{\partial\,p^2}-E\left(\frac{\partial^2\,\ln(1-p)}{\partial\,p^2}X\right)\\
                Var\left(\frac{1}{p-1}X\right)&=\frac{r}{p^2}-E\left(-\frac{1}{(p-1)^2}X\right)\\
                \frac{1}{(p-1)^2}Var(X)&=\frac{r}{p^2}-\frac{r}{(p-1)p}\\
                Var(X)&=\frac{r(p-1)^2}{p^2}-\frac{r(p-1)p}{p^2}\\
                &=\frac{r(p^2-2p+1)-r(p^2-p)}{p^2}\\
                &=\frac{r(1-p)}{p^2}. 
            \end{align*}
            \item From the definition of gamma distribution, we have
            \begin{align*}
                f(x|\alpha ,\beta )=\frac{1}{\Gamma(\alpha )\beta ^{\alpha }}x^{\alpha -1}e^{-\frac{x}{\beta }}. 
            \end{align*}
            Then, 
            \begin{align*}
                EX&=\int_0^\infty x\frac{1}{\Gamma(\alpha )\beta ^{\alpha }}x^{\alpha -1}e^{-\frac{x}{\beta }}dx\\
                &=\frac{1}{\Gamma(\alpha )\beta ^{\alpha }}\int_0^\infty x^{\alpha }e^{-\frac{x}{\beta }}dx
            \end{align*}
            Substituting $t=\frac{x}{\beta}$, $dx=\beta dt$, we have
            \begin{align*}
                EX&=\frac{1}{\Gamma(\alpha )\beta ^{\alpha }}\int_0^\infty \beta^\alpha t^{\alpha }e^{-t}\beta dt\\
                &=\frac{\beta}{\Gamma(\alpha )}\int_0^\infty t^{\alpha}e^{-t}dt\\
                &=\frac{\beta}{\Gamma(\alpha )}\Gamma(\alpha+1)\\
                &=\frac{\beta\alpha\Gamma(\alpha)}{\Gamma(\alpha)}=\alpha\beta. 
            \end{align*}
            And 
            \begin{align*}
                EX^2&=\int_0^\infty x^2\frac{1}{\Gamma(\alpha )\beta ^{\alpha }}x^{\alpha -1}e^{-\frac{x}{\beta }}dx\\
                &=\frac{1}{\Gamma(\alpha )\beta ^{\alpha }}\int_0^\infty x^{\alpha +1}e^{-\frac{x}{\beta }}dx
            \end{align*}
            Same as above, substituting $t=\frac{x}{\beta}$, $dx=\beta dt$, we have
            \begin{align*}
                EX^2&=\frac{1}{\Gamma(\alpha )\beta ^{\alpha }}\int_0^\infty \beta^{\alpha +1}t^{\alpha +1}e^{-t}\beta dt\\
                &=\frac{\beta^2}{\Gamma(\alpha )}\int_0^\infty t^{\alpha +1}e^{-t}dt\\
                &=\frac{\beta^2}{\Gamma(\alpha )}\Gamma(\alpha+2)\\
                &=\frac{\beta^2(\alpha+1)\alpha\Gamma(\alpha)}{\Gamma(\alpha)}=(\alpha+1)\alpha\beta^2. 
            \end{align*}
            So, the variance is
            \[
                Var(X)=EX^2-(EX)^2=(\alpha+1)\alpha\beta^2-\alpha^2\beta^2=\alpha\beta^2.
            \]
        \end{enumerate}
    \end{solution}
    
    \begin{exercise}
        The \emph{Pareto} distribution, with parameters $\alpha$ and $\beta$, has p.d.f.
        \[
            f(x)=\frac{\beta\alpha^\beta}{x^{\beta+1}}, \quad \alpha<x<\infty, \quad, \alpha>0,\quad \beta>0. 
        \]
        \begin{enumerate}[(a)]
            \item Verify that $f(x)$ is a p.d.f. 
            \item Derive the mean and variance of this distribution.
            \item Prove that the variance does not exist if $\beta\leq2$. 
        \end{enumerate}
    \end{exercise}

    \begin{solution}
        \begin{enumerate}[(a)]
            \item \begin{enumerate}[i)]
                \item Because $\alpha$, $\beta$, $x$ are all positive, $f(x)>0$. 
                \item \begin{align*}
                    \int_\alpha^\infty \frac{\beta\alpha^\beta}{x^{\beta+1}}dx&=\beta\alpha^\beta\int_\alpha^\infty x^{-\beta-1}dx\\
                    &=\beta\alpha^\beta\left.(-\beta^{-1}x^{-\beta})\right|_\alpha^\infty\\
                    &=\beta\alpha^\beta\frac{1}{\beta}\alpha^{-\beta}=1. 
                \end{align*}
                So, $f(x)$ is a p.d.f.
            \end{enumerate}
            \item From the definition, we have
            \begin{align*}
                EX&=\int_\alpha^\infty x\frac{\beta\alpha^\beta}{x^{\beta+1}}dx\\
                &=\beta\alpha^\beta\int_\alpha^\infty x^{-\beta}dx\\
                &=\beta\alpha^\beta\left.\left(-\frac{1}{\beta-1}x^{-\beta+1}\right)\right|_\alpha^\infty\\
                &=\beta\alpha^\beta\frac{1}{\beta-1}\alpha^{-\beta+1}\\
                &=\frac{\alpha\beta}{\beta-1}.
            \end{align*}
            \begin{align*}
                EX^2&=\int_\alpha^\infty x^2\frac{\beta\alpha^\beta}{x^{\beta+1}}dx\\
                &=\beta\alpha^\beta\int_\alpha^\infty x^{-\beta+1}dx\\
                &=\beta\alpha^\beta\left.\left(-\frac{1}{\beta-2}x^{-\beta+2}\right)\right|_\alpha^\infty\\
                &=\beta\alpha^\beta\frac{1}{\beta-2}\alpha^{-\beta+2}\\
                &=\frac{\alpha^2\beta}{\beta-2}. 
            \end{align*}
            So, 
            \begin{align*}
                Var(X)&=EX^2-(EX)^2\\
                &=\frac{\alpha^2\beta}{\beta-2}-\frac{\alpha^2\beta^2}{(\beta-1)^2}\\
                &=\frac{\alpha^2\beta^3-2\alpha^2\beta^2+\alpha^2\beta-\alpha^2\beta^3+2\alpha^2\beta^2}{(\beta-2)(\beta-1)^2}\\
                &=\frac{\alpha^2\beta}{(\beta-2)(\beta-1)^2}. 
            \end{align*}
            \item If $\beta\leq2$, then $\beta-2<0$, so $EX^2=\infty$, hence, the variance does not exist.
        \end{enumerate}
    \end{solution}

    \setcounter{exer}{36}
    \begin{exercise}
        Show that if $f(x)$ is a p.d.f., symmetric about $x=0$, then $\mu$ is the median of the location-scale pdf $(1/\sigma)f((x-\mu)/\sigma)$, $-\infty<x<\infty$. 
    \end{exercise}

    \begin{solution}
        Because $f(x)$ is symmetric about $x=0$, for $\varepsilon>0$, 
        \[
            \frac{1}{\sigma}f\left(\frac{(\mu+\varepsilon)-\mu}{\sigma}\right)=\frac{1}{\sigma}f\left(\frac{\varepsilon}{\sigma}\right)=\frac{1}{\sigma}f\left(\frac{-\varepsilon}{\sigma}\right)=\frac{1}{\sigma}f\left(\frac{(\mu-\varepsilon)-\mu}{\sigma}\right), 
        \]
        So, $\frac{1}{\sigma}f\left(\frac{x-\mu}{\sigma}\right)$ is symmetric about $x=\mu$. Then
        \begin{align*}
            \int_\mu^\infty \frac{1}{\sigma}f\left(\frac{x-\mu}{\sigma}\right)dx&=\int_{-\infty}^\mu \frac{1}{\sigma}f\left(\frac{x-\mu}{\sigma}\right)dx
        \end{align*}
        Therefore, $\mu$ is median. 
    \end{solution}

    \setcounter{exer}{39}
    \begin{exercise}
        Let $f(x)$ be any p.d.f. with mean $\mu$ and variance $\sigma^2$. Show how to create a location-scale family based on $f(x)$ such that the standard p.d.f. of the family, say $f^*(x)$, has mean $0$ and variance $1$.
    \end{exercise}

    \begin{solution}
        Let $Z=\frac{X-\mu}{\sigma}$, then
        \[EZ=\frac{1}{\sigma}E(X-\mu)=0\]
        \[Var(Z)=Var\left(\frac{X-\mu}{\sigma}\right)=\frac{Var(X-\mu)}{\sigma^2}=\frac{\sigma^2}{\sigma^2}=1. \]
        And $X=\sigma Z+\mu$, $\frac{\partial X}{\partial Z}=\sigma$. So, 
        \[
            f_Z(z)=f_X(\sigma z+\mu)\sigma. 
        \]
    \end{solution}

    \setcounter{exer}{43}
    \begin{exercise}
        For any random variable $X$ for which $EX^2$ and $E|X|$ exist, show that $P(|X|\geq b)$ does not exceed either $EX^2/b^2$ or $E|X|/b$, where $b$ is a positive constant. If $f(x)=e^{-x}$ for $x>0$, show that one bound is better when $b=3$ and the other when $b=\sqrt{2}$. (Notice Markov's inequality in Miscellanea 3.8.2.)
    \end{exercise}

    \begin{solution}
        Because $g(X)=|X|$ is nonnegative, Using Chebyshev's inequality, we have
        \[P(|X|\geq b)\leq\frac{E|X|}{b}. \]
        And $|X|\geq b\Rightarrow X^2\geq b^2$, $g(x)=x^2$, is nonnegative. So, 
        \[P(|X|\geq b)=P(X^2\geq b^2)\leq\frac{EX^2}{b^2}. \]
        For $f(x)=e^{-x}$, $X\sim exp(1)$, 
        \[E|X|=EX=\lambda^{-1}=1. \]
        \[EX^2=Var(X)+(EX)^2=\lambda^{-2}+1=2. \]
        When $b=3$, 
        \[
            \frac{EX^2}{b^2}=\frac{2}{9}<\frac{E|X|}{b}=\frac{1}{3},
        \]
        So, $EX^2/b^2$ is better. 

        When $b=\sqrt{2}$, 
        \[
            \frac{EX^2}{b^2}=\frac{2}{2}>\frac{E|X|}{b}=\frac{1}{\sqrt{2}},
        \]
        So, $E|X|/b$ is better. 
    \end{solution}
    
\end{document}